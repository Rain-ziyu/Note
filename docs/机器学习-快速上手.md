# 决策树

首先，我们将概述机器学习模型的工作原理及其使用方式。

## 我们假设一个场景：
你的表弟通过房地产投机赚了数百万美元。由于您对数据科学感兴趣，他愿意与您成为业务合作伙伴。他将提供资金，您将提供预测各种房屋价值的模型。
你问你的表弟他过去是如何预测房地产价值的，他说这只是直觉。但更多的询问表明，他从过去看到的房子中确定了价格模式，并使用这些模式来预测他正在考虑的新房子。

机器学习的工作方式相同。 我们将从一个名为 Decision Tree （决策树）的模型开始。有更高级的模型可以给出更准确的预测。但决策树很容易理解，它们是数据科学中最佳模型的基础模块。


为简单起见，我们将从最简单的决策树开始。

![](./images/machineLearn/image.png)

该决策树只将房屋分为两类。每一类中房屋的预测价格是同一类别中房屋的历史平均价格。

我们使用数据来决定如何将房屋分成两组，然后再次确定每组中的预测价格。 从数据中按模式分类的这一步称为拟合或训练模型。用于拟合模型的数据称为训练数据。

模型如何拟合的细节（例如，如何拆分数据）非常复杂，我们将其保存以备后用。拟合模型后，您可以将其应用于新数据以预测其他房屋的价格。

### 改进决策树

![](./images/machineLearn/twotree.png)

以上两个决策树中的哪一个更有可能通过拟合房地产训练数据而产生？

左侧的决策树（决策树 1）可能更有意义，因为它捕捉到了这样一个现实，即卧室较多的房屋往往比卧室较少的房屋售价更高。 该模型的最大缺点是它没有捕捉到影响房价的大多数因素，例如浴室数量、地块大小、位置等。

您可以使用具有更多“拆分”的树来捕获更多因子。这些被称为 “更深” 的树。这个树还考虑每栋房屋地块总大小等。
如下所示：

![](./images/machineLearn/deepertree.png)

您可以通过跟踪决策树来预测任何房屋的价格，始终选择与该房屋特征相对应的路径。房屋的预测价格位于树的底部。 我们进行预测的底部点称为叶子。

叶子上的拆分和值将由数据决定，因此是后续我们将要查看处理数据。

# 随机森林（Random Forests）

决策树往往会带来深度选择的问题。具有大量叶子的深树将过度拟合，因为每个预测仅来自其叶子处的几座房屋的历史数据。但是，叶子很少的浅树性能会很差，因为它无法在原始数据中捕获尽可能多的区别。

即使是当今最复杂的建模技术也面临着欠拟合和过拟合之间的这种紧张关系。但是，许多模型都有自己的解决方法，可以带来更好的性能。我们将以随机森林为例。

随机森林使用许多树，它通过平均每个组件树的预测来做出预测。它通常比单个决策树具有更好的预测准确性，并且默认参数就可以有不错的表现。如果您深入学习建模，您可以学习更多性能更好的模型，但其中许多模型对于参数很敏感。

## 示例

前提：在数据加载结束时，我们定义以下变量：
* train_X 
* val_X 
* train_y 
* val_y 

```python
import pandas as pd
    
# Load data
melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'
melbourne_data = pd.read_csv(melbourne_file_path) 
# Filter rows with missing values
melbourne_data = melbourne_data.dropna(axis=0)
# Choose target and features
y = melbourne_data.Price
melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 
                        'YearBuilt', 'Lattitude', 'Longtitude']
X = melbourne_data[melbourne_features]

from sklearn.model_selection import train_test_split

# split data into training and validation data, for both features and target
# The split is based on a random number generator. Supplying a numeric value to
# the random_state argument guarantees we get the same split every time we
# run this script.
train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)
```


以下代码我们构建一个随机森林模型类似于我们在Scikit -learn中构建决策树的方式类似 - 这次使用RandomForestRegressor类，而不是DecisionTreeRegressor。

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

forest_model = RandomForestRegressor(random_state=1)
forest_model.fit(train_X, train_y)
melb_preds = forest_model.predict(val_X)
print(mean_absolute_error(val_y, melb_preds))
```

最终输出： 191669.7536453626

可能还有进一步改进的空间，但这比最佳决策树最终的的250,000已经有了改进。当我们更改单个决策树的最大深度时，有一些参数使您可以更改随机森林的性能。但是，随机森林模型的最佳特征之一是，即使没有这种调整，它们通常也可以合理地工作。

# 中级机器学习

基于以上两种简单的模型算法的使用我们对如何初步训练一个模型有了简单的了解。
但是实际的模型开发、优化过程中是更复杂的，比如：
* 处理真实数据集中常见的数据类型（缺失值、分类变量），
* 设计管道（pipelines ）以提高机器学习代码的质量，
* 使用高级技术进行模型验证（交叉验证），
* 构建（XGBoost） 的先进模型，以及
* 避免常见和重要的数据科学错误（泄漏）。

## 处理数据缺失值

在实际开发时数据缺失是非常常见的，比如：
两居室的房子不包括第三间卧室的大小值。
调查受访者可以选择不分享其收入。
如果您尝试使用具有缺失值的数据构建模型，大多数机器学习库（包括 scikit-learn）都会出现错误。因此，您需要选择以下三种常见策略之一。

### 1.删除具有缺失值的列

![](./images/machineLearn/Sax80za.png)

这种方法一般不推荐，除非某一列中的大多数都没有值，否则使用此方法将导致模型失去对许多（可能有用的！）信息。 作为一个极端的例子，请考虑一个具有10,000行的数据集，其中一个重要的列中仅缺少一个有效值。这种方法将完全丢弃该列！

### 2.补充缺失值

![](./images/machineLearn/4BpnlPA.png)

使用一些数字填充缺失值。 例如，我们可以为其填充每列平均值。但是这并不是固定的，平均值有时候甚至会不如直接删除缺失列，但这并不代表补充缺失值是更优的，而是我们补充的值不合适，我们可能会采取其他方式来填充值，比如众数等

在大多数情况下，补充的值不会完全正确，但是通常会比完全放弃列获得更准确的模型。

### 3.拓展列然后补充缺失值

补充缺失值是标准方法，通常效果很好。但是，估算的值可能会系统地高于或低于其实际值（该值未收集到数据集中）。或着缺少值的行可能在某种程度上是唯一的。在这种情况下，您的模型将通过考虑最初缺少哪些值来做出更好的预测。
![](./images/machineLearn/UWOyg4a.png)
在这种方法中，我们像以前一样将缺失的值进行估算补充。 此外，对于原始数据集中的每列丢失条目，我们添加了一个新列，以显示该列缺失值。

在某些情况下，这将有意义地改善结果。某些情况下，这也可能无济于事。

## 分类数据

分类变量只采用有限的枚举值，比如：
* 在一项调查中，询问您多久吃一次早餐并提供四个选项：“从不”、“很少”、“大多数日子”或“每天”。 在这种情况下，数据是分类的，因为响应属于一组固定的类别。
* 如果人们回答关于他们拥有哪个品牌的汽车的调查，回答将分为 “本田”、“丰田 ”和 “福特 ”等类别。 在这种情况下，数据也是分类的。

而此时针对这些缺失值我们也需要处理，主要有以下三种方法：

### 删除分类数据列
仅当列不包含有用信息时，此方法才会有效。
### 为分类数据编码（序数编码（Ordinal Encoding））

将每个唯一值分配给不同的整数。
![](./images/machineLearn/tEogUAr.png)
使用此方法我们假定类别的顺序：“从不”（0） < “很少” （1） < “大多数天数” （2） < “每天” （3）。

这个假设在这个例子中是有道理的，因为这些类别有一个比较明显的排名一句。 并非所有分类变量在值中都有明确的排序，因此我们将这些分类变量称为有序变量。 对于基于树的模型（如决策树和随机森林），您可以通过预期序数来为数据编码，一般可以很好地处理序数变量。

### 独热编码

该词直译英文（One-Hot Encoding），独热编码通过创建新列，来指示原始数据中存在（或不存在）每个可能的值。 为了理解这一点，我们将通过一个示例来说明。

![](./images/machineLearn/TW5m0aJ.png)
在原始数据集中，“Color” 是一个分类变量，具有三个类别：“Red”、“Yellow” 和 “Green”。 相应的 one-hot 编码为每个可能的值创建一个新列，原始数据集中的每一行都将被对应起来。 如果原始值为 “Red”，我们在 “Red” 列中放置一个 1;如果原始值为 “Yellow”，则在 “Yellow” 列中输入 1，依此类推。

与Oldinal编码（序数编码）相反，独热编码不假定类别的排序。 因此，如果分类数据中没有明确的排序（例如，“红色”既不是 也不是 ，则可以期望这种方法特别有效。 我们将这种没有内在排名的分类变量称为名义变量）。

但是如果分类变量具有大量枚举值（即，通常不会将其用于变量以上超过15个不同的值），则One-Hot编码通常不能很好地表现。

针对为分类数据编码还存在特殊情况，即有时候训练数据与验证数据由于数据集的影响，两者会具有不同的分类变量，即训练数据中的分类变量少于验证数据，即 训练数据中没有验证数据中的某些分类信息。
测试一般会细分出两种解决方法：
#### 放弃有问题的分类列
以下代码示例可以有效地将训练数据中没有验证数据中的某些分类信息的列打印出来
```python
# Categorical columns in the training data
object_cols = [col for col in X_train.columns if X_train[col].dtype == "object"]

# Columns that can be safely ordinal encoded
good_label_cols = [col for col in object_cols if
                   set(X_valid[col]).issubset(set(X_train[col]))]

# Problematic columns that will be dropped from the dataset
bad_label_cols = list(set(object_cols)-set(good_label_cols))

print('Categorical columns that will be ordinal encoded:', good_label_cols)
print('\nCategorical columns that will be dropped from the dataset:', bad_label_cols)
```
#### 


## Pipelines

管道是保持数据预处理和建模代码井井有条的一种简单方法。 具体来说，就是使用管道来捆绑预处理和建模这些步骤，让您可以像使用单个步骤一样使用整个捆绑管道。

许多人在没有管道的情况下将模型组合在一起，但使用管道可以为我们带来一些重要的好处。这些包括：

* 清理代码：在预处理的每个步骤中逐步处理每一条数据可能会变得混乱。 使用管道，您无需在每个步骤中手动跟踪训练和验证数据。
* 更少的错误：减少因使用错误的处理方式或忘记了预处理步骤导致的错误。
* 更容易生产应用：将模型从原型过渡到可大规模部署的模型可能非常困难。 我们不会在这里讨论许多相关问题，但管道可以提供一些帮助让我们减轻困难。
* 为模型验证提供更多可选项：比如交叉验证等。

### 定义预处理步骤（ Define Preprocessing Steps）
就像流水线管道一样将预处理和建模步骤捆绑在一起，我们使用ColumnTransformer类将不同的预处理步骤捆绑在一起。 
* 填补数值数据中的缺失值，并且
* 填补分类数据的缺失的值，并将单热编码应用于对应数据。
以下代码：
```python
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder

# Preprocessing for numerical data
numerical_transformer = SimpleImputer(strategy='constant')

# Preprocessing for categorical data
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])
```
### 定义模型

我们使用熟悉的 RandomForestRegressor 类定义一个随机森林模型。
```python
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(n_estimators=100, random_state=0)
```
### 创建和评估流水线管道（Pipeline）

我们使用 Pipeline 类来定义一个pipeline 去绑定预处理和建模步骤的管道。 有几点重要事项需要注意：

* 使用管道，我们可以使得预处理训练数据和训练模型在一行代码中。 （相比之下，如果没有管道，我们必须分步进行插补、one-hot encoding 和模型训练。 如果我们必须同时处理数值和分类变量，这将变得特别混乱！）
* 使用管道，我们将 X_valid 中未处理的特征（features ）提供给 predict() 命令，管道会在生成预测之前自动预处理这些特征。 （但是，如果没有管道，我们必须记住在进行预测之前预处理验证数据。）

```python
from sklearn.metrics import mean_absolute_error

# Bundle preprocessing and modeling code in a pipeline
my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                              ('model', model)
                             ])

# Preprocessing of training data, fit model 
my_pipeline.fit(X_train, y_train)

# Preprocessing of validation data, get predictions
preds = my_pipeline.predict(X_valid)

# Evaluate the model
score = mean_absolute_error(y_valid, preds)
print('MAE:', score)
```