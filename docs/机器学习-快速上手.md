# 决策树

首先，我们将概述机器学习模型的工作原理及其使用方式。

## 我们假设一个场景：
你的表弟通过房地产投机赚了数百万美元。由于您对数据科学感兴趣，他愿意与您成为业务合作伙伴。他将提供资金，您将提供预测各种房屋价值的模型。
你问你的表弟他过去是如何预测房地产价值的，他说这只是直觉。但更多的询问表明，他从过去看到的房子中确定了价格模式，并使用这些模式来预测他正在考虑的新房子。

机器学习的工作方式相同。 我们将从一个名为 Decision Tree （决策树）的模型开始。有更高级的模型可以给出更准确的预测。但决策树很容易理解，它们是数据科学中最佳模型的基础模块。


为简单起见，我们将从最简单的决策树开始。

![](./images/machineLearn/image.png)

该决策树只将房屋分为两类。每一类中房屋的预测价格是同一类别中房屋的历史平均价格。

我们使用数据来决定如何将房屋分成两组，然后再次确定每组中的预测价格。 从数据中按模式分类的这一步称为拟合或训练模型。用于拟合模型的数据称为训练数据。

模型如何拟合的细节（例如，如何拆分数据）非常复杂，我们将其保存以备后用。拟合模型后，您可以将其应用于新数据以预测其他房屋的价格。

### 改进决策树

![](./images/machineLearn/twotree.png)

以上两个决策树中的哪一个更有可能通过拟合房地产训练数据而产生？

左侧的决策树（决策树 1）可能更有意义，因为它捕捉到了这样一个现实，即卧室较多的房屋往往比卧室较少的房屋售价更高。 该模型的最大缺点是它没有捕捉到影响房价的大多数因素，例如浴室数量、地块大小、位置等。

您可以使用具有更多“拆分”的树来捕获更多因子。这些被称为 “更深” 的树。这个树还考虑每栋房屋地块总大小等。
如下所示：

![](./images/machineLearn/deepertree.png)

您可以通过跟踪决策树来预测任何房屋的价格，始终选择与该房屋特征相对应的路径。房屋的预测价格位于树的底部。 我们进行预测的底部点称为叶子。

叶子上的拆分和值将由数据决定，因此是后续我们将要查看处理数据。

# 随机森林（Random Forests）

决策树往往会带来深度选择的问题。具有大量叶子的深树将过度拟合，因为每个预测仅来自其叶子处的几座房屋的历史数据。但是，叶子很少的浅树性能会很差，因为它无法在原始数据中捕获尽可能多的区别。

即使是当今最复杂的建模技术也面临着欠拟合和过拟合之间的这种紧张关系。但是，许多模型都有自己的解决方法，可以带来更好的性能。我们将以随机森林为例。

随机森林使用许多树，它通过平均每个组件树的预测来做出预测。它通常比单个决策树具有更好的预测准确性，并且默认参数就可以有不错的表现。如果您深入学习建模，您可以学习更多性能更好的模型，但其中许多模型对于参数很敏感。

## 示例

前提：在数据加载结束时，我们定义以下变量：
* train_X 
* val_X 
* train_y 
* val_y 

```python
import pandas as pd
    
# Load data
melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'
melbourne_data = pd.read_csv(melbourne_file_path) 
# Filter rows with missing values
melbourne_data = melbourne_data.dropna(axis=0)
# Choose target and features
y = melbourne_data.Price
melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 
                        'YearBuilt', 'Lattitude', 'Longtitude']
X = melbourne_data[melbourne_features]

from sklearn.model_selection import train_test_split

# split data into training and validation data, for both features and target
# The split is based on a random number generator. Supplying a numeric value to
# the random_state argument guarantees we get the same split every time we
# run this script.
train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)
```


以下代码我们构建一个随机森林模型类似于我们在Scikit -learn中构建决策树的方式类似 - 这次使用RandomForestRegressor类，而不是DecisionTreeRegressor。

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

forest_model = RandomForestRegressor(random_state=1)
forest_model.fit(train_X, train_y)
melb_preds = forest_model.predict(val_X)
print(mean_absolute_error(val_y, melb_preds))
```

最终输出： 191669.7536453626

可能还有进一步改进的空间，但这比最佳决策树最终的的250,000已经有了改进。当我们更改单个决策树的最大深度时，有一些参数使您可以更改随机森林的性能。但是，随机森林模型的最佳特征之一是，即使没有这种调整，它们通常也可以合理地工作。

# 中级机器学习

基于以上两种简单的模型算法的使用我们对如何初步训练一个模型有了简单的了解。
但是实际的模型开发、优化过程中是更复杂的，比如：
* 处理真实数据集中常见的数据类型（缺失值、分类变量），
* 设计管道（pipelines ）以提高机器学习代码的质量，
* 使用高级技术进行模型验证（交叉验证），
* 构建（XGBoost） 的先进模型，以及
* 避免常见和重要的数据科学错误（泄漏）。

## 处理数据缺失值

在实际开发时数据缺失是非常常见的，比如：
两居室的房子不包括第三间卧室的大小值。
调查受访者可以选择不分享其收入。
如果您尝试使用具有缺失值的数据构建模型，大多数机器学习库（包括 scikit-learn）都会出现错误。因此，您需要选择以下三种常见策略之一。

### 1.删除具有缺失值的列

![](./images/machineLearn/Sax80za.png)

这种方法一般不推荐，除非某一列中的大多数都没有值，否则使用此方法将导致模型失去对许多（可能有用的！）信息。 作为一个极端的例子，请考虑一个具有10,000行的数据集，其中一个重要的列中仅缺少一个有效值。这种方法将完全丢弃该列！

### 2.补充缺失值

![](./images/machineLearn/4BpnlPA.png)

使用一些数字填充缺失值。 例如，我们可以为其填充每列平均值。但是这并不是固定的，平均值有时候甚至会不如直接删除缺失列，但这并不代表补充缺失值是更优的，而是我们补充的值不合适，我们可能会采取其他方式来填充值，比如众数等

在大多数情况下，补充的值不会完全正确，但是通常会比完全放弃列获得更准确的模型。

### 3.拓展列然后补充缺失值

补充缺失值是标准方法，通常效果很好。但是，估算的值可能会系统地高于或低于其实际值（该值未收集到数据集中）。或着缺少值的行可能在某种程度上是唯一的。在这种情况下，您的模型将通过考虑最初缺少哪些值来做出更好的预测。
![](./images/machineLearn/UWOyg4a.png)
在这种方法中，我们像以前一样将缺失的值进行估算补充。 此外，对于原始数据集中的每列丢失条目，我们添加了一个新列，以显示该列缺失值。

在某些情况下，这将有意义地改善结果。某些情况下，这也可能无济于事。

## 分类数据

分类变量只采用有限的枚举值，比如：
* 在一项调查中，询问您多久吃一次早餐并提供四个选项：“从不”、“很少”、“大多数日子”或“每天”。 在这种情况下，数据是分类的，因为响应属于一组固定的类别。
* 如果人们回答关于他们拥有哪个品牌的汽车的调查，回答将分为 “本田”、“丰田 ”和 “福特 ”等类别。 在这种情况下，数据也是分类的。

而此时针对这些缺失值我们也需要处理，主要有以下三种方法：

### 删除分类数据列
仅当列不包含有用信息时，此方法才会有效。
### 为分类数据编码（序数编码（Ordinal Encoding））

将每个唯一值分配给不同的整数。
![](./images/machineLearn/tEogUAr.png)
使用此方法我们假定类别的顺序：“从不”（0） < “很少” （1） < “大多数天数” （2） < “每天” （3）。

这个假设在这个例子中是有道理的，因为这些类别有一个比较明显的排名一句。 并非所有分类变量在值中都有明确的排序，因此我们将这些分类变量称为有序变量。 对于基于树的模型（如决策树和随机森林），您可以通过预期序数来为数据编码，一般可以很好地处理序数变量。

### 独热编码

该词直译英文（One-Hot Encoding），独热编码通过创建新列，来指示原始数据中存在（或不存在）每个可能的值。 为了理解这一点，我们将通过一个示例来说明。

![](./images/machineLearn/TW5m0aJ.png)
在原始数据集中，“Color” 是一个分类变量，具有三个类别：“Red”、“Yellow” 和 “Green”。 相应的 one-hot 编码为每个可能的值创建一个新列，原始数据集中的每一行都将被对应起来。 如果原始值为 “Red”，我们在 “Red” 列中放置一个 1;如果原始值为 “Yellow”，则在 “Yellow” 列中输入 1，依此类推。

与Oldinal编码（序数编码）相反，独热编码不假定类别的排序。 因此，如果分类数据中没有明确的排序（例如，“红色”既不是 也不是 ，则可以期望这种方法特别有效。 我们将这种没有内在排名的分类变量称为名义变量）。

但是如果分类变量具有大量枚举值（即，通常不会将其用于变量以上超过15个不同的值），则One-Hot编码通常不能很好地表现。

针对为分类数据编码还存在特殊情况，即有时候训练数据与验证数据由于数据集的影响，两者会具有不同的分类变量，即训练数据中的分类变量少于验证数据，即 训练数据中没有验证数据中的某些分类信息。
测试一般会细分出两种解决方法：
#### 放弃有问题的分类列
以下代码示例可以有效地将训练数据中没有验证数据中的某些分类信息的列打印出来
```python
# Categorical columns in the training data
object_cols = [col for col in X_train.columns if X_train[col].dtype == "object"]

# Columns that can be safely ordinal encoded
good_label_cols = [col for col in object_cols if
                   set(X_valid[col]).issubset(set(X_train[col]))]

# Problematic columns that will be dropped from the dataset
bad_label_cols = list(set(object_cols)-set(good_label_cols))

print('Categorical columns that will be ordinal encoded:', good_label_cols)
print('\nCategorical columns that will be dropped from the dataset:', bad_label_cols)
```
#### 


## Pipelines

管道是保持数据预处理和建模代码井井有条的一种简单方法。 具体来说，就是使用管道来捆绑预处理和建模这些步骤，让您可以像使用单个步骤一样使用整个捆绑管道。

许多人在没有管道的情况下将模型组合在一起，但使用管道可以为我们带来一些重要的好处。这些包括：

* 清理代码：在预处理的每个步骤中逐步处理每一条数据可能会变得混乱。 使用管道，您无需在每个步骤中手动跟踪训练和验证数据。
* 更少的错误：减少因使用错误的处理方式或忘记了预处理步骤导致的错误。
* 更容易生产应用：将模型从原型过渡到可大规模部署的模型可能非常困难。 我们不会在这里讨论许多相关问题，但管道可以提供一些帮助让我们减轻困难。
* 为模型验证提供更多可选项：比如交叉验证等。

### 定义预处理步骤（ Define Preprocessing Steps）
就像流水线管道一样将预处理和建模步骤捆绑在一起，我们使用ColumnTransformer类将不同的预处理步骤捆绑在一起。 
* 填补数值数据中的缺失值，并且
* 填补分类数据的缺失的值，并将单热编码应用于对应数据。
以下代码：
```python
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder

# Preprocessing for numerical data
numerical_transformer = SimpleImputer(strategy='constant')

# Preprocessing for categorical data
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])
```
### 定义模型

我们使用熟悉的 RandomForestRegressor 类定义一个随机森林模型。
```python
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(n_estimators=100, random_state=0)
```
### 创建和评估流水线管道（Pipeline）

我们使用 Pipeline 类来定义一个pipeline 去绑定预处理和建模步骤的管道。 有几点重要事项需要注意：

* 使用管道，我们可以使得预处理训练数据和训练模型在一行代码中。 （相比之下，如果没有管道，我们必须分步进行插补、one-hot encoding 和模型训练。 如果我们必须同时处理数值和分类变量，这将变得特别混乱！）
* 使用管道，我们将 X_valid 中未处理的特征（features ）提供给 predict() 命令，管道会在生成预测之前自动预处理这些特征。 （但是，如果没有管道，我们必须记住在进行预测之前预处理验证数据。）

```python
from sklearn.metrics import mean_absolute_error

# Bundle preprocessing and modeling code in a pipeline
my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                              ('model', model)
                             ])

# Preprocessing of training data, fit model 
my_pipeline.fit(X_train, y_train)

# Preprocessing of validation data, get predictions
preds = my_pipeline.predict(X_valid)

# Evaluate the model
score = mean_absolute_error(y_valid, preds)
print('MAE:', score)
```




## 交叉验证

机器学习是一个迭代过程。在这个过程中您将面临以下选择：使用哪些预测变量、使用哪些类型的模型、向这些模型提供哪些参数等。之前我们通过使用验证（或保留）数据来衡量模型质量，以数据驱动的方式做出了这些选择。

但是这种方法也有一些缺点。要体现这一点，假设您有一个包含 5000 行的数据集。您通常会将大约 20% 的数据保留为验证数据集，即 1000 行。但这在确定模型分数时会产生一定的随机性。也就是说，一个模型可能在这一组 1000 行上表现良好，但它在不同的 1000 行上不准确。

在极端情况下，您可以想象验证集中只有 1 行数据。如果您比较所有模型，哪一个模型会对单个数据点做出最佳预测将主要取决于运气！

一般来说，验证集越大，我们衡量模型质量的随机性（又名 “噪声”） 就越少，它就越可靠。但不幸的是，我们只能通过从训练数据中删除行来获得较大的验证集，而较小的训练数据集也意味着更差的模型！

### 什么是交叉验证

在交叉验证中，我们对数据的不同子集运行建模过程，以获得模型质量的多个度量。

例如，我们可以先将数据分成 5 个部分，每个部分占整个数据集的 20%。在本例中，我们已经将数据分成了 5 个 “folds”。
![](./images/machineLearn/9k60cVA.png)

然后，我们为每个组运行一次实验：
* 在实验 1 中，我们使用第一个折叠作为验证（或保留）集，使用其他所有内容作为训练数据。这为我们提供了基于 20% 保留集的模型质量度量。
* 在实验 2 中，我们保留第二个折叠的数据（并使用除第二个折叠之外的所有内容来训练模型）。然后，使用维持集来获取模型质量的第二次估计值。
* 我们重复此过程，将每个弃牌使用一次作为维持集。综上所述，100% 的数据在某个时候被用作保留，我们最终得到了一个基于数据集中所有行的模型质量度量（即使我们没有同时使用所有行）。

### 什么场景下使用交叉验证

交叉验证可以更准确地衡量模型质量，这在您做出大量建模决策时尤为重要。但是，运行时间可能需要更长的时间，因为它会估计多个模型（每个折叠一个模型）。

那么，考虑到这些权衡，何时应该使用该方法呢？
* 对于小型数据集，额外的计算负担并不是什么大问题，此时可以进行交叉验证。
* 对于较大的数据集，单个验证集就足够了。您的代码将运行得更快，并且您可能有足够的数据，因此几乎不需要重用其中一些数据来保持模型可靠性。

对于大型数据集与小型数据集的构成，没有简单的阈值。但是，如果您的模型需要几分钟或更短的时间才能运行，则可能值得切换到交叉验证。

或者，您可以运行交叉验证，并查看每个实验组的分数是否接近。如果每个实验产生相同的结果，则单个验证集可能就足够了。

### 示例

```python
import pandas as pd

# Read the data
data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')

# Select subset of predictors
cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']
X = data[cols_to_use]

# Select target
y = data.Price
```

然后，我们定义一个管道，该管道使用补充缺失值的方式来解决缺失值，并使用随机森林模型进行预测。

虽然可以在没有管道的情况下进行交叉验证，但这非常困难！使用管道将使代码非常简单。

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

my_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),
                              ('model', RandomForestRegressor(n_estimators=50,
                                                              random_state=0))
                             ])
```

在模拟时我们使用 scikit-learn 中的 cross_val_score（） 函数获得交叉验证分数。 我们使用 参数cv 来设置折叠数。

```python


from sklearn.model_selection import cross_val_score

# Multiply by -1 since sklearn calculates *negative* MAE
scores = -1 * cross_val_score(my_pipeline, X, y,
                              cv=5,
                              scoring='neg_mean_absolute_error')

print("MAE scores:\n", scores)

```

上文参数解释：scoring 参数：选择要报告的模型质量度量方式：在本例中，我们选择了**负的**平均绝对误差 （MAE）。在 [scikit-learn](https://scikit-learn.org/stable/modules/model_evaluation.html) 的文档显示了所有的可选项列表。

我们指定取反的 MAE 有点令人惊讶。scikit-learn 有一个约定，其中定义了所有指标数字越高越好。在这里使用负数可以使它们与该约定保持一致，尽管负 MAE 在其他地方几乎闻所未闻。

我们通常需要单一的模型质量度量方式来比较替代模型。因此，我们取实验的平均值。
### 小结

使用交叉验证可以更好地衡量模型质量，并具有清理代码的额外好处：请注意，我们不再需要跟踪单独的训练集和验证集。因此，特别是对于小型数据集，这是一个很好的改进！

## 梯度提升（Gradient Boosting 、XGBoost）

在之前的大部分内容中，我们都使用随机森林法进行预测，该方法通过对许多决策树的预测求平均值，比单个决策树获得更好的性能。

我们将随机森林法称为 “集成法”的一种。根据定义，集成法结合了多个模型的预测（例如，在随机森林的中是几棵树）。

梯度提升是一种通过循环去迭代的方式将模型添加到集成中的方法。

它首先使用单个模型初始化集成，该模型的预测可能非常离谱。（即使它的预测非常不准确，后续通过在集成模型中继续补充来解决这些错误。）

然后，我们开始循环：

* 首先，我们使用当前集成模型为数据集中的每个观测值生成预测。预测结果是这样生成的，即我们将集成中所有模型的预测相加。
* 将这些预测用于计算损失函数（例如，均方误差）。
* 然后，我们使用 损失函数来拟合新模型并将其添加到集成中。具体来说，我们确定模型参数，以便将这个新模型添加到集成中将减少损失。（旁注：“gradient boosting”中的“gradient”指的是我们将在损失函数上使用梯度下降法来确定这个新模型中的参数。
* 最后，我们将新模型添加到 ensemble 中，然后 一直重复该过程

![](./images/machineLearn/MvCGENh.png)

### 示例

```python
import pandas as pd
from sklearn.model_selection import train_test_split

# Read the data
data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')

# Select subset of predictors
cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']
X = data[cols_to_use]

# Select target
y = data.Price

# Separate data into training and validation sets
X_train, X_valid, y_train, y_valid = train_test_split(X, y)
```

在后续示例中，我们将使用 XGBoost 库。XGBoost 代表极端梯度提升，这是梯度提升的一种实现，具有一些专注于性能和速度的附加功能。（Scikit-learn 有另一个版本的梯度提升，但 XGBoost 有一些技术优势。）

我们导入 XGBoost 的 scikit-learn API （xgboost.XGBRegressor 的 Exports）。 这允许我们构建和拟合模型，就像我们在 scikit-learn 中所做的那样。 正如您将在输出中看到的那样，XGBRegressor 类具有许多可调参数 -- 您很快就会了解这些参数！

```python
from xgboost import XGBRegressor

my_model = XGBRegressor()
my_model.fit(X_train, y_train)
# 对模型进行预测和评估。
from sklearn.metrics import mean_absolute_error

predictions = my_model.predict(X_valid)
print("Mean Absolute Error: " + str(mean_absolute_error(predictions, y_valid)))
```
### 参数调优

XGBoost 有一些参数会极大地影响准确性和训练速度。您应该了解的第一个参数是：
- n_estimators 
    指定上述建模周期的次数。 它等于我们在集成中包含的模型数量。
    *  值太低会导致欠拟合，从而导致对训练数据和测试数据的预测不准确。
    *  值过高会导致过度拟合，这会导致对训练数据进行准确的预测，但对测试数据的预测不准确（这是我们关心的）。

    典型值范围为 100-1000，但这在很大程度上取决于下面讨论的 learning_rate 参数。

    以下是通过 ensemble 设置模型数量的代码：
    ```python
    my_model = XGBRegressor(n_estimators=500)
    my_model.fit(X_train, y_train)
    ```
- early_stopping_rounds
    early_stopping_rounds 提供了一种自动查找 n_estimators 理想值的方法。early_stopping_rounds会导致模型在验证分数(即用于验证模型表现的方法)停止提高时停止迭代，即使我们没有达到 n_estimators 的硬停止状态。 为 n_estimators 设置一个较高的值，然后使用 early_stopping_rounds 找到停止迭代的最佳时间是明智的。

    由于随机机会有时会导致单轮验证分数没有提高，因此您需要指定一个数字，说明在停止之前允许多少轮直接恶化。 设置 early_stopping_rounds=5 是一个合理的选择。 在这种情况下，我们会在连续 5 轮验证分数恶化后停止。

    使用 early_stopping_rounds 时，您还需要留出一些数据来计算验证分数 - 这是通过设置 eval_set 参数来完成的。

    经验证较新版本的XGBoost将early_stopping_rounds参数放到了XGBRegressor的构造函数中

    下面是我们可以修改上面的示例以应用early_stopping_rounds：
    ```python
        my_model = XGBRegressor(early_stopping_rounds=5, n_estimators=500)
        my_model.fit(X_train, y_train, 
                    
                    eval_set=[(X_valid, y_valid)],
                    verbose=False)
    ```
    以后如果要用所有数据拟合模型，将 n_estimators 设置为在提前停止运行时发现最优的任何值。

- learning_rate

    我们可以将每个模型的预测值乘以一个小数字（称为学习率），然后再将它们相加，而不是简单地将每个组件模型的预测相加来获得预测。
    这意味着我们添加到 ensemble 中的每棵树对我们的帮助较小。 因此，我们可以为 n_estimators 设置更高的值而不会过拟合。 如果我们使用提前停止，将自动确定适当的树木数量。

    一般来说，较小的学习率（learning_rate）和大量的估计器（estimators）将产生更准确的 XGBoost 模型，尽管模型也需要更长的时间来训练，因为它在整个周期中进行更多的迭代。 默认情况下，XGBoost 设置 learning_rate=0.1。

    修改上面的示例以应用学习率参数：
    ```python
    my_model = XGBRegressor(early_stopping_rounds=5, n_estimators=1000, learning_rate=0.05)
    my_model.fit(X_train, y_train, 
                eval_set=[(X_valid, y_valid)], 
                verbose=False)
    ```
- n_jobs

    在考虑运行时的较大数据集上，您可以使用并行性来更快地构建模型。 通常将参数 n_jobs 设置为等于计算机上的内核数。 对于较小的数据集，这无济于事。
    生成的模型不会更好，因此对拟合时间进行微调通常只会分散注意力。但是，它在大型数据集中非常有用，否则您将花费很长时间等待 fit 命令。
    以下是修改后的示例：
    ```python
    my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)
    my_model.fit(X_train, y_train, 
                early_stopping_rounds=5, 
                eval_set=[(X_valid, y_valid)], 
                verbose=False)
    ```
### 总结
XGBoost 是一个领先的软件库，用于处理标准表格数据（您存储在 Pandas DataFrames 中的数据类型，而不是图像和视频等更奇特的数据类型）。通过仔细的参数调整，您可以训练高度准确的模型。

## Data Leakage 数据泄露

### 介绍
数据泄漏常发生在当您的训练数据包含有关目标的信息时，当模型用于实际预测时，类似的数据却无法提供。这会导致在训练数据（甚至可能是验证数据）时表现较好，但模型在生产中表现不佳。

换句话说，泄漏会导致模型看起来准确，直到你实际开始使用模型做出决策时模型变得非常不准确。

泄漏主要有两种类型：目标泄漏和训练测试污染（ target leakage and train-test contamination.）。

#### Target leakage  目标泄漏

使用在实际预测期间不可用的信息会导致过度拟合，即模型在训练和验证数据上表现非常好，但在生产环境中表现不佳。

因此这是非常重要的去根据数据获得的时间或时间顺序（因果）来考虑目标泄漏是否存在，而不仅仅是某个特征是否有助于做出良好的预测。

举个例子会有所帮助。假设您要预测谁会患肺炎。原始数据的前几行如下所示：

![](./images/machineLearn/1746539433090.png)

人们在患肺炎后服用抗生素药物以恢复。原始数据显示这些列之间有很强的关系，但在确定 got_pneumonia 的值后，took_antibiotic_medicine 经常也会跟随发生变化。这就是目标泄漏。

该模型将看到 took_antibiotic_medicine 的值为 False 的任何人都没有肺炎。由于验证数据与训练数据来自同一来源（都具有是否服用抗生素的特征数据），因此该模式将在验证中基本是重复于预测目标的，并且模型将具有很高的验证（或交叉验证）分数。

但是一旦到了实际预测时的用户都是没有服用抗生素的，但是不能因此预测其未来是否会得肺炎，因此该模型的表现会大打折扣。

![](./images/machineLearn/y7hfTYe.png)

#### Train-Test Contamination 训练-测试数据污染
当你在区分训练数据与验证数据时，会发生各种不同类型的泄漏。

回想一下，验证是用于衡量模型如何处理以前未考虑过的数据的指标。如果验证数据影响预处理行为，则可以以微妙的方式破坏整个验证过程。这有时称为训练测试污染。

例如，假设您在调用 train_test_split（） 之前运行预处理（例如为缺失值使用拟合 imputer进行填充），即你还没有分割数据就进行了缺失值的填充，此时填充的值很有可能会受测试数据影响。最终结果？您的模型在测试数据上可能会获得良好的验证分数，让您对它充满信心，但在部署它以做出决策时，性能会很差

毕竟，您将验证或测试数据中的数据都纳入到了预测过程中，因此它不能很好的推广到新数据，也可能在该特定数据上表现良好。当您进行更复杂的特征工程时，这个问题变得更加微妙（也更加危险）。

如果您的验证基于简单的训练-测试拆分，请从任何类型的拟合（fit）中完全排除验证数据，包括用于拟合时的预处理步骤。如果您使用 scikit-learn 管道，这会更容易。使用交叉验证时，在管道内进行预处理更为重要！

### 例子

例如我们将使用有关信用卡应用程序的数据集，并跳过基本的数据设置代码。 最终结果是，有关每个信用卡应用程序的信息都存储在 DataFrame X 中。 我们将使用它来预测 Series y 即信用卡的申请结果。

![](./images/machineLearn/1746541195782.png)

由于这是一个较小的数据集，我们将使用交叉验证来确保模型质量的准确度量。
```python
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

# Since there is no preprocessing, we don't need a pipeline (used anyway as best practice!)
my_pipeline = make_pipeline(RandomForestClassifier(n_estimators=100))
cv_scores = cross_val_score(my_pipeline, X, y, 
                            cv=5,
                            scoring='accuracy')

print("Cross-validation accuracy: %f" % cv_scores.mean())
```
经过简单验证我们得到了Cross-validation accuracy: 0.981052
根据经验，您会发现一般情况下很少能找到在 98% 时间内准确的模型。这种情况会发生，但这种情况并不常见，我们应该更仔细地检查数据是否有目标泄漏。

以下是数据摘要，您也可以在 data （数据） 选项卡下找到：

* card：如果接受信用卡申请，则为 1，如果不接受，则为 0
* reports：主要贬损报告的数量
* age：年龄 n 岁加 12 岁
* income： 年收入（除以 10,000）
* share： 信用卡月消费与年收入的比率
* expenditure：平均每月信用卡支出
* owner：拥有房屋时为 1，出租时为 0
* selfempl：如果是个体经营者，则为 1，如果不是，则为 0
* dependents：1 + 依赖项数量
* months： 在当前地址居住的月份
* majorcards：持有的主要信用卡数量
* active：活跃信用账户的数量

其中一些变量看起来可疑。 例如，expenditure是指在这张卡上或申请前使用的卡上的支出吗？

此时，基于基本数据进行比较可能非常有帮助：
```python
expenditures_cardholders = X.expenditure[y]
expenditures_noncardholders = X.expenditure[~y]

print('Fraction of those who did not receive a card and had no expenditures: %.2f' \
      %((expenditures_noncardholders == 0).mean()))
print('Fraction of those who received a card and had no expenditures: %.2f' \
      %(( expenditures_cardholders == 0).mean()))
```
经过对基础数据中的比例进行比较，我们不难发现没有收到卡的每个人都没有消费，而收到卡的人中只有 2% 没有消费。我们的模型似乎具有很高的准确率也就不足为奇了。但这似乎也是一个目标泄漏的情况，其中 SPENDING 可能是指他们申请的卡上的支出。

由于share部分由expenditure决定，因此也应将其排除在外。 变量 active 和 majorcards 不太清楚，但从描述来看，它们听起来很令人担忧。 在大多数情况下，如果您无法追踪创建数据、收集数据的那些人以了解更多详细信息，但是安全总比后悔好。

我们将运行一个处理后没有目标泄漏的模型，如下所示：

```python
# Drop leaky predictors from dataset
potential_leaks = ['expenditure', 'share', 'active', 'majorcards']
X2 = X.drop(potential_leaks, axis=1)

# Evaluate the model with leaky predictors removed
cv_scores = cross_val_score(my_pipeline, X2, y, 
                            cv=5,
                            scoring='accuracy')

print("Cross-val accuracy: %f" % cv_scores.mean())
```

Cross-val accuracy: 0.830919

这个准确性要低很多，这可能会令人失望。 但是，我们可以预期它在用于新应用程序时大约 80% 的时间是正确的，而泄漏模型可能会比这糟糕得多（尽管它在交叉验证中的明显分数更高）。

### 结论

在许多大型数据科学应用程序中，数据泄露可能会造成数百万美元的错误。 仔细分离训练和验证数据可以防止训练-测试污染，而管道可以帮助实现这种分离。 同样，谨慎、常识和数据探索相结合可以帮助识别目标泄漏。
# 特征工程

机器学习模型的最重要步骤之一就是构建一个好的特种工程。为了得到一个好的特征工程我们需要：
* 使用互信息（mutual information）相互确定哪些特征最重要
* 在多个实际问题域中找出特征
* 使用目标编码（target encoding）对高基数（可能值非常多）分类进行编码
* 使用 K-Means 聚类（k-means clustering）创建细分特征
* 使用主成分分析（ principal component analysis /PCA）将数据集的变体分解为特征

## 特征工程的目标
首先特征工程的目标只是使您的数据更适合手头的问题。
考虑以下场景：在“表面温度”相关度量中，有这些指标：如热指数和风寒。这些量试图根据空气温度、湿度和风速来测量人类感知的温度，这些都是我们可以直接测量的。您可以将表观温度视为一种特征工程的结果，试图使观察到的数据与我们真正关心事情的更相关，即外面的实际感觉如何！

我们可以使用特征工程来：
* 提高模型的预测性能
* 减少计算或数据需求
* 提高结果的可解释性

## 特征工程的指导原则

要使特征有用，它必须与模型学习的目标建立一定的关系。例如，线性模型只能学习线性关系。因此，在使用线性模型时，您的目标是转换特征以使它们与目标的关系呈线性关系。

这里的关键思想是，您对于特征的转换实质上会成为模型本身的一部分。假设您试图从一侧的 Length 预测方形地块的 Price。将线性模型直接拟合到 这里 会得到很差的结果：这是因为关系并不是线性的。

但是，如果我们将 Length 特征进行平方以获得 'Area'，我们将创建一个线性关系。将 Area 添加到特征集意味着该线性模型现在可以拟合抛物线。换句话说，对特征进行平方使线性模型能够拟合平方特征。

![](./images/machineLearn/BLRsYOK.png)

从上面的例子，应该会告诉你为什么在特征工程上投入的时间回报如此之高。无论您的模型无法学习什么关系，您都可以通过转换来为其提供能力。在开发特征集时，请考虑模型可以使用哪些信息来实现其最佳性能。

### 具体实例

为了证明以上想法，我们将通过向数据集添加一些合成特征来了解如何提高随机森林模型的预测性能。
在Concrete 数据集包含各种混凝土配方和所得产品的抗压强度，这是衡量这种混凝土可以承受多少载荷的指标。该数据集的任务是预测给定混凝土公式的抗压强度。

```python
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score

df = pd.read_csv("../input/fe-course-data/concrete.csv")
df.head()
```
![](./images/machineLearn/1747316002748.png)

您可以在此处看到各种混凝土中的各种成分。我们稍后将通过添加一些从这些特征派生的额外合成特征来演示如何帮助模型学习它们之间的重要关系。

我们首先通过在未增强的数据集上训练模型来作为基准。这将帮助我们确定我们的新功能是否真的有用。

在特征工程流程开始时，建立这样的基准是很好的做法。基准分数可以帮助确定新功能是否值得保留，或者是否应该丢弃它们并可能尝试其他功能。

```python
X = df.copy()
y = X.pop("CompressiveStrength")

# Train and score baseline model
baseline = RandomForestRegressor(criterion="absolute_error", random_state=0)
baseline_score = cross_val_score(
    baseline, X, y, cv=5, scoring="neg_mean_absolute_error"
)
baseline_score = -1 * baseline_score.mean()

print(f"MAE Baseline Score: {baseline_score:.4}")
```
基准模型的 MAE 分数为  8.232。我们将使用此分数来比较我们在数据集上添加的合成特征的性能。

如果你曾经在家做饭，你可能知道食谱中成分的比例通常比它们的绝对数量更能预测食谱的结果。那么我们可以推断，上述特征的比率将是 CompressiveStrength 的良好预测因子。

下面的单元格向数据集添加了三个新的比率特征。

```python
X = df.copy()
y = X.pop("CompressiveStrength")

# Create synthetic features
X["FCRatio"] = X["FineAggregate"] / X["CoarseAggregate"]
X["AggCmtRatio"] = (X["CoarseAggregate"] + X["FineAggregate"]) / X["Cement"]
X["WtrCmtRatio"] = X["Water"] / X["Cement"]

# Train and score model on dataset with additional ratio features
model = RandomForestRegressor(criterion="absolute_error", random_state=0)
score = cross_val_score(
    model, X, y, cv=5, scoring="neg_mean_absolute_error"
)
score = -1 * score.mean()

print(f"MAE Score with Ratio Features: {score:.4}")
```

最终结果是：MAE Score with Ratio Features: 7.948

果然，性能得到了提升！这证明这些新的比率特征向模型提供了它以前没有检测到的重要信息。

## Mutual Information 互信息
第一次遇到新的数据集有时会让人感到不知所措。您可能会看到成百上千个功能，甚至没有描述可供参考。你甚至不知道该从哪里开始？

一个很好的第一步是使用特征效用指标来构建排名，该指标是衡量特征与目标之间关联的函数。然后，您可以选择一小部分最有用的功能进行初步开发，并可以让您的时间得到充分利用。

我们将使用的指标称为 “mutual information”。互信息很像相关性，因为它测量两个量之间的关系。互信息的优势在于它可以检测任何类型的关系，而相关性只能检测线性关系。

互信息是一个很好的通用指标，在功能开发开始时特别有用，特别是你可能还不知道要使用什么模型的时候。

互信息是：易于使用和解释，计算效率高，理论上有根据，抗过拟合，以及能够检测任何类型数据之间的关系


### 互信息及其衡量标准

互信息从不确定性的角度来描述关系。两个量之间的互信息 （MI） 是用于衡量一个已知的特征量可以在多大程度上减少另一个量的不确定性的量度。如果您知道某项功能的价值，那么您对目标的信心会有多大？

以下是 Ames Housing 数据中的一个示例。该图显示了房屋的外部质量与其售价之间的关系。每个点代表一个房子。

![](./images/machineLearn/X12ARUK.png)

从图中我们可以看出，知道 ExterQual 的值应该可以让你对相应的 SalePrice 范围更加确定 —— ExterQual 的每个类别都倾向于将 SalePrice 集中在某个范围内。
ExterQual（外部质量评估）与SalePrice（销售价格）之间的互信息，可理解为通过分析ExterQual的四个等级（如TA、Gd、Ex、Fa），所获得的销售价格不确定性的平均降低程度。

（技术说明：这里所称的"不确定性"是通过信息论中的"熵"来量化的。一个变量的熵可以通俗理解为：描述该变量某个具体取值时，平均需要多少个二进制的"是或否"问题。需要提问的次数越多，说明对该变量的不确定性越大。而互信息则表征某个特征（如外部质量评估ExterQual）预期能为目标变量（如销售价格SalePrice）解答多少个这类问题

#### Interpreting Mutual Information Scores  解释互信息得分
数量之间可能的最小互信息为 0.0。当 MI 为零时，表示量是独立的：两者都不能告诉你关于另一个的任何事情。相反，理论上 MI 没有上限。但在实践中，高于 2.0 左右的值并不常见。（互信息是一个对数，所以它增加得非常慢。

下图将让您了解 MI 值如何与特征与目标的关联类型和程度相对应。

![](./images/machineLearn/Dt75E1f.png)

左图：随着特征和目标之间的依赖性变得更紧密，相互信息会增加。右图：相互信息可以捕获任何类型的关联（不仅是线性的，像是相关性。）

以下是应用互信息时要记住的一些注意事项：
* 互信息（MI）能够帮助量化评估某个特征（如外部质量评估ExterQual）单独作为预测因子时，对目标变量（如销售价格SalePrice）的潜在解释能力。
* 某些特征（如房屋层数）可能在与其他特征（如建筑面积）产生协同效应时具有显著预测力，但其单独存在时解释能力可能有限。互信息（MI）无法捕捉这类特征间的交互效应，因其本质上是基于单变量分析的评估指标。
* 特征的实际有用性取决于您配合使用的模型。特征仅在其与目标的关系是模型可以学习的关系时才有用。仅仅因为某个特征具有较高的 MI 分数并不意味着您的模型将能够使用该信息执行任何作。您可能需要先转换特征才能显示关联。

#### 示例

Automobile数据集由 1985 年的 193 辆汽车车型组成。此数据集的目标是从汽车的 23 个特征（如 make、body_style 和 horsepower）预测汽车的 price（目标）。在此示例中，我们将使用互信息对特征进行排名，并通过数据可视化来展示结果。
```python
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

plt.style.use("seaborn-v0_8-whitegrid")

df = pd.read_csv("../input/fe-course-data/autos.csv")
df.head()
```
scikit-learn中用于互信息（MI）的算法对离散特征和连续特征的处理方式有所不同。因此，您需要明确告知算法哪些特征是离散的，哪些是连续的。一个经验法则是：任何必须使用浮点型（float dtype）的数据通常不被视为离散特征。对于分类变量（object或category dtype），可以通过对它们进行标签编码（Label Encoding）将其视为离散特征（您可以在我们的《分类变量》课程中复习标签编码的相关知识）。

通过以下方式区分哪些列是离散的哪些是连续的
```python
X = df.copy()
y = X.pop("price")

# Label encoding for categoricals
for colname in X.select_dtypes("object"):
    X[colname], _ = X[colname].factorize()

# All discrete features should now have integer dtypes (double-check this before using MI!)
discrete_features = X.dtypes == int
```

Scikit-learn 在其 feature_selection 模块中有两个互信息指标：一个用于实值目标 （mutual_info_regression），一个用于分类目标 （mutual_info_classif）。我们的目标 price 是实值。下面计算我们特征的 MI 分数，并将它们包装在一个DataFrame 中。

```python
from sklearn.feature_selection import mutual_info_regression

def make_mi_scores(X, y, discrete_features):
    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)
    mi_scores = pd.Series(mi_scores, name="MI Scores", index=X.columns)
    mi_scores = mi_scores.sort_values(ascending=False)
    return mi_scores

mi_scores = make_mi_scores(X, y, discrete_features)
mi_scores[::3]  # show a few features with their MI scores
```

![](./images/machineLearn/1747319355438.png)

现在构建一个条形图，使效果更明显：

```python
def plot_mi_scores(scores):
    scores = scores.sort_values(ascending=True)
    width = np.arange(len(scores))
    ticks = list(scores.index)
    plt.barh(width, scores)
    plt.yticks(width, ticks)
    plt.title("Mutual Information Scores")


plt.figure(dpi=100, figsize=(8, 5))
plot_mi_scores(mi_scores)
```
![](./images/machineLearn/1747319490449.png)


通过数据可视化对可用性排名通常对我们很有帮助。让我们仔细看看其中的几个。
```python
sns.relplot(x="curb_weight", y="price", data=df);
``` 
![](./images/machineLearn/1747319696796.png)

正如我们所料，高分的 curb_weight 特征与目标 price 表现出很强的关系。
```python
sns.lmplot(x="horsepower", y="price", hue="fuel_type", data=df);
``` 
尽管fuel_type（燃料类型）特征的互信息（MI）得分较低，但从图中可以看出，该特征在horsepower（马力）特征内部清晰地区分了具有不同价格趋势的两个群体。这表明fuel_type实际上存在交互效应（interaction effect），因此可能并非完全不重要。在仅凭MI得分判定某个特征不重要之前，有必要探究其潜在的交互效应——此时领域知识（domain knowledge）能提供重要指导。

数据可视化是特征工程工具箱的重要补充。除了互信息等效用指标外，此类可视化还可以帮助您发现数据中的重要关系。
![](./images/machineLearn/1747320075423.png)


# Pandas
## 使用Pandas进行创建、读取、写入数据

### 开始
要使用 pandas，您通常从以下代码行开始。
```python
import pandas as pd
```

### 创建数据
pandas 中有两个核心对象：DataFrame 和 Series。

#### DataFrame

DataFrame 是一个表。它包含一个由各个条目组成的数组，每个条目都有一个特定的值。每个条目对应于一行（或记录）和一列。
```python
pd.DataFrame({'Yes': [50, 21], 'No': [131, 2]})
```
![](./images/machineLearn/1746598906805.png)

DataFrame 中的值不限于整数。例如，下面是一个 DataFrame，其值是字符串：
```python
pd.DataFrame({'Bob': ['I liked it.', 'It was awful.'], 'Sue': ['Pretty good.', 'Bland.']})
```
使用 pd.DataFrame() 构造函数来生成 DataFrame 对象。声明新 列 的语法是一个字典，其键是列名（在本例中为 Bob 和 Sue），其值是条目列表。这是构造新 DataFrame 的标准方法，也是您最有可能遇到的方法。

上述构造函数为列标签分配值，但只使用从 0 （0， 1， 2， 3， ...） 开始的升序计数来表示行标签。有时这没问题，但通常我们会希望自己分配这些标签。
在DataFrame 中使用的行标签列表称为 Index。我们可以在构造函数中使用 index 参数为其赋值：
```python
pd.DataFrame({'Bob': ['I liked it.', 'It was awful.'], 
              'Sue': ['Pretty good.', 'Bland.']},
             index=['Product A', 'Product B'])

```

![](./images/machineLearn/1746600155523.png)

#### Series
相比之下，Series 是一系列数据值。如果 DataFrame 是一个表，那么 Series 是一个列表。事实上，您可以创建一个只包含一个列的列表：
![](./images/machineLearn/1746600355130.png)
从本质上讲，Series 是 DataFrame 的单个列。因此，您可以像以前一样使用 index 参数将行标签分配给 Series。但是，Series 没有列名，它只有一个整体 name：

Series 和 DataFrame 密切相关。我们可以将 DataFrame 视为实际上只是一堆 “粘在一起” 的 Series 。

### 读取数据文件

能够手动创建 DataFrame 或 Series 非常方便。但是，大多数时候，我们实际上不会手动创建我们自己的数据。相反，我们将使用已经存在的数据。

数据可以以多种不同的形式和格式存储。到目前为止，其中最基本的是不起眼的 CSV 文件。当您打开 CSV 文件时，您会收到如下所示的内容：
```csv
Product A,Product B,Product C,
30,21,9,
35,34,1,
41,11,11
```
因此，CSV 文件是一个由逗号分隔的值表。因此得名：“Comma-Separated Values”，或 CSV。
现在让我们把我们的玩具数据集放在一边，看看当我们将其读入 DataFrame 时，真正的数据集是什么样子的。我们将使用 pd.read_csv() 函数将数据读入 DataFrame。这是这样的：
```python
wine_reviews = pd.read_csv("../winemag-data-130k-v2.csv")
# 我们可以使用 shape 属性来检查生成的 DataFrame 有多大：
wine_reviews.shape
```
(129971, 14)
因此，我们的新 DataFrame 有 130,000 条记录，分为 14 个不同的列。差不多有 200 万个条目！

pd.read_csv() 函数功能丰富，您可以指定 30 多个可选参数。例如，您可以在之前的数据集中看到 CSV 文件具有内置索引（序号），而 pandas 不会自动选择该索引。要让 pandas 将该列用于索引（而不是在创建新列用于索引），我们可以指定 index_col。

![](./images/machineLearn/1746604550758.png)

## Indexing, Selecting & Assigning(索引、选择和分配)

### 简介

选择要处理的 pandas DataFrame 或 Series 的特定值几乎是您将运行的任何数据作中的隐式步骤，因此在 Python 中处理数据时，您需要学习的第一件事是如何快速有效地选择与您相关的数据点。

### 原生形式访问
原生 Python 对象提供了索引数据的好方法。Pandas 继承了所有这些功能，这有助于我们快速上手。
```python
import pandas as pd
reviews = pd.read_csv("../input/wine-reviews/winemag-data-130k-v2.csv", index_col=0)
pd.set_option('display.max_rows', 5)
reviews
```
![](./images/machineLearn/1746607591545.png)

在 Python 中，我们可以通过将对象作为属性来访问对象的属性。例如，一个 book 对象可能有一个 title 属性，我们可以通过调用 book.title 来访问它。pandas DataFrame 中的列的工作方式大致相同。

因此，要访问 reviews 的 country 属性，我们可以使用：
```python
reviews.country
```
如果我们有一个 Python 字典，我们可以使用索引 （[]） 运算符访问它的值。我们可以对 DataFrame 中的列执行相同的作：
```python
reviews['country']
```
这是从 DataFrame 中选择特定 Series 的两种方法。它们在语法上都不比另一个更有效或更差，但索引运算符 [] 确实具有一个优点，即它可以处理带有保留字符的列名（例如，如果我们有一个 `country providence` 列，reviews.country providence 将不起作用）。

pandas 中的序列（Series）看起来不是有点像一本花哨的词典吗？差不多是这样，所以，要向下取到单个特定值，我们只需要再次使用索引运算符 [] 也就不足为奇了：
```python
reviews['country'][0]
```
### 在 pandas 中的索引

索引运算符和属性选择我们已经讲过了，因为它们的工作方式与 Python 生态系统的其余部分一样。作为新手，这使它们易于上手和使用。但是，pandas 有自己的访问器运算符 loc 和 iloc。这是更高级的使用方式。

#### 基于索引的选择
使用Pandas 索引两种范例之一。首先是基于索引的选择：根据数据中的数字位置选择数据。iloc 方法就是如此实现。
要选择 DataFrame 中的第一行数据，我们可以使用以下内容：
```python
reviews.iloc[0]
```
![](./images/machineLearn/1746608223927.png)
loc 和 iloc 都是行优先、列第二的。这与我们在原生 Python 中所做的相反，即列优先、行第二。这意味着是用这种方式检索行稍微容易一些，而获取检索列稍微困难一些。要获取 iloc 的列，我们可以执行以下方式：
```python
reviews.iloc[:, 0]
```
`:` 运算符来自原生 Python，表示“一切”。但是，当与其他选择器结合使用时，它可用于指示值范围。例如，要从第一行、第二行和第三行中选择 country 列，我们可以这样做：
```python
reviews.iloc[:3, 0]
```

![](./images/machineLearn/1746610175166.png)
最后，值得知道的是，负数可以在选择中使用。这将从值的末尾开始向前计数。例如，以下是数据集的最后五个元素。

#### 基于标签的选择
属性选择的第二种范例是 loc 运算符：基于标签的选择。在此范式中，重要的是根据数据值进行索引，而不是其位置。

例如，要获取 reviews 中的第一个条目，我们现在将执行以下作：
```python
reviews.loc[0, 'country']
```
iloc 在概念上比 loc 简单，因为它忽略了数据集的索引。当我们使用 iloc 时，我们将数据集视为一个大矩阵（一个二维列表），我们必须按位置索引。相比之下，loc 使用索引中的信息来完成其工作。由于你的数据集通常具有有意义的索引，因此使用 loc 通常更容易。例如，这里有一个作使用 loc 要容易得多：
```python
reviews.loc[:, ['taster_name', 'taster_twitter_handle', 'points']]
```

#### 在 loc 和 iloc 之间进行选择

在 loc 和 iloc 之间进行选择或转换时，有一个 “陷阱” 值得牢记，即这两种方法使用的索引方案略有不同。

iloc 使用 Python stdlib 索引方案，其中包含范围的第一个元素，排除最后一个元素。所以 0:10 将选择条目 0,...,9。loc，同时，包括索引。所以 0:10 将选择条目 0,...,10。

为什么会有这样的变化？请记住，loc 可以索引任何 stdlib 类型：例如字符串。如果我们有一个索引值为 Apples, ..., Potatoes, ... 的 DataFrame，并且我们想选择“苹果和土豆之间的所有按字母顺序排列的水果选择”，那么索引 df.loc['Apples':'Potatoes'] 比索引 df.loc['Apples', 'Potatoet'] 之类的内容要方便得多（t 在字母表中 s 之后）。

#### Manipulating the index（操作索引）

上文提到基于列标签方式的选择（Label-based selection）可以基于索引列中的标签提取数据。至关重要的是，我们使用的索引不是不可变的。我们可以以我们认为合适的任何列作索引。
此时基于列标签方式的选择使用时就可以在行范围确定时使用新指定的索引列，包括字符串类型。
![](./images/machineLearn/image%20copy.png)

#### 按条件选择

到目前为止，我们一直在使用 DataFrame 本身的结构属性为各种数据步幅编制索引。然而，为了对数据做一些有趣的事情，我们通常需要根据条件提出问题。

例如，假设我们特别关注意大利生产的优于平均水平的葡萄酒。
我们可以先检查每种葡萄酒是否是意大利葡萄酒：
```python
reviews.country == 'Italy'
```
![](./images/machineLearn/image%20copy%202.png)
此作根据每条记录的 country 生成一系列 True/False 布尔值。 然后可以在 loc 中使用这个结果来选择相关数据：
```python
reviews.loc[reviews.country == 'Italy']
```
![](./images/machineLearn/image%20copy%203.png)

此 DataFrame 有 ~20,000 行。原版有 ~130,000 个。这意味着大约 15% 的葡萄酒来自意大利。

我们还想知道哪些比平均水平好。葡萄酒的审查范围为 80 到 100 分，因此这可能意味着葡萄酒至少获得 90 分。

我们可以使用 & 符号 （&） 将两个问题放在一起进行按条件筛选：
```python
reviews.loc[(reviews.country == 'Italy') & (reviews.points >= 90)]
```
![](./images/machineLearn/image%20copy%204.png)

假设我们会购买任何意大利制造或评级高于平均水平的葡萄酒。为此，我们使用竖线 （|）注意这里直接使用or会报错：

![](./images/machineLearn/image%20copy%205.png)

Pandas 带有一些内置的条件选择器，我们将在此处重点介绍其中两个。

第一个是 isin。isin 允许您选择其值“位于”值列表中的数据。例如，以下是我们如何使用它来仅选择来自意大利或法国的葡萄酒：

```python
reviews.loc[reviews.country.isin(['Italy', 'France'])]
```

![](./images/machineLearn/image%20copy%206.png)

第二个是 isnull （及其同伴 notnull）。这些方法允许您突出显示为空（或非）空 （NaN） 的值。例如，要过滤掉数据集中缺少价格标签的葡萄酒，我们可以这样做：
```python
reviews.loc[reviews.price.notnull()]
```
![](./images/machineLearn/image%20copy%207.png)

### Assigning data¶ 分配数据

反过来将数据分配给 DataFrame 很容易。您可以分配一个常量值：

```pythons
reviews['critic'] = 'everyone'
reviews['critic']
```
或者使用迭代来赋值：
```python
reviews['index_backwards'] = range(len(reviews), 0, -1)
reviews['index_backwards']
```

## Summary Functions and Maps (摘要函数和数据映射)

之前，我们学习了如何从 DataFrame 或 Series 中选择相关数据。我们的从数据表中提取正确的数据对于完成工作至关重要。

但是，数据并不总是以我们想要的格式从内存中出来。有时我们自己必须提前重新格式化数据来时期可以被用于数据训练。本教程将介绍我们将学习涵盖多种不同的数据处理操作用于格式化数据，以获得 “恰到好处” 的输入。

### 摘要函数

Pandas 提供了许多简单的 “summary functions” （不是官方名称），它们可以以一些有用的方式重组数据。例如，考虑 describe() 方法：

```python
reviews.points.describe()
```

此方法可以为给定列的属性生成一些高级摘要。它会给予类型感知，这意味着其输出会根据输入的数据类型而变化。上面的输出仅对数字数据有意义;对于 String Data，我们得到的是：
![](./images/machineLearn/1746693000753.png)

如果您想获取有关 DataFrame 或 Series 中列的一些特定简单摘要统计数据，通常 pandas 都会有一个对应函数实现。

例如，要查看分配的分数的平均值（例如，平均评分的葡萄酒表现如何），我们可以使用 mean() 函数：
```python
reviews.points.mean()
```

要查看唯一值的列表，我们可以使用 unique() 函数：
```python
reviews.taster_name.unique()
```

要查看唯一值的列表以及它们在数据集中出现的频率，我们可以使用 value_counts() 方法：
```python
reviews.taster_name.value_counts()
```
### Maps（映射）

映射是一个从数学中借用的术语，指的是一个函数，它采用一组值并将它们“映射”到另一组值。在数据科学中，我们经常需要从现有数据创建新的表示形式，或者将数据从现在的格式转换为我们以后想要的格式。地图是处理这项工作的工具，因此它们对于完成您的工作极为重要！

在pandas您将经常使用两种映射方法。

#### map（）
map（） 是第一个，也是稍微简单的一个。例如，假设我们想将葡萄酒获得的分数以平均分为0点重新计算。我们可以按如下方式执行此作：
```python
review_points_mean = reviews.points.mean()
reviews.points.map(lambda p: p - review_points_mean)
```
传递给 map() 的函数应该是 Series 中的单个值（在上面的示例中为 point 值），并返回该值的转换版本。map() 返回一个新的 Series，其中所有值都已由您的函数转换。

#### apply()
apply（） 是等效于我们想通过在每一行上调用自定义方法来转换整个 DataFrame。

```python
def remean_points(row):
    row.points = row.points - review_points_mean
    return row

reviews.apply(remean_points, axis='columns')
```

![](./images/machineLearn/1746693947156.png)

如果我们用 axis='index' 调用 reviews.apply()，那么我们需要给出一个函数来转换每一列，而不是传递一个函数来转换每一行。

** 请注意，map() 和 apply() 分别返回新的、转换后的 Series 和 DataFrames。他们不会修改他们被调用的原始数据。如果我们查看 reviews 的第一行，我们可以看到它仍然具有其原始的 points 值。 **

![](./images/machineLearn/1746694271964.png)

Pandas 提供了许多常见的 Map作作为内置作。例如，这是用 points 求每一列的平均值更快方法：
```python


review_points_mean = reviews.points.mean()
reviews.points - review_points_mean


```

在此代码中，我们在左侧的大量值（Series 中的所有内容）和右侧的单个值（平均值）之间执行作。Pandas 查看了这个表达式，并发现我们必须打算从数据集中的每个值中减去该平均值。

Pandas 也会明白如果我们在等长的 Series 之间执行这些作该怎么办。例如，在数据集中组合国家和地区信息的一种简单方法是执行以下作：

```python


reviews.country + " - " + reviews.region_1


```

这些运算符比 map() 或 apply() 更快，因为它们使用了 pandas 内置的加速。所有标准 Python 运算符（>、<、== 等）都以这种方式工作。

但是，它们不如 map() 或 apply() 灵活，后者可以做更高级的事情，比如应用条件逻辑，这是单独通过加法和减法无法完成的。

## 分组和排序

之前学习的映射允许我们转换 DataFrame 或 Series 中的数据，一次转换整个列的一个值。但是，我们通常希望对数据进行分组，然后针对数据所在的组执行特定操作。

我们将使用 groupby()作来执行此作。 此外我们还将介绍一些其他主题，例如为 DataFrame 编制索引的更复杂的方法，以及如何对数据进行排序。

### Groupwise analysis  将数据进行分组分析

到目前为止，我们一直在大量使用的一个函数是 value_counts() 函数，用于统计分组后的每组数据条数。我们可以通过执行以下作来实现与value_counts() 等效的效果：
```python
reviews.groupby('points').points.count()
```

![](./images/machineLearn/1746709825561.png)

groupby() 方法创建了一组视图，将葡萄酒分值相同的分配到一组。然后，对于这些组中的每一个，我们抓取 points() 列并计算它出现的次数。 value_counts() 就是这么实现的。
除此之外，我们还可以对这些数据使用之前使用的任何摘要函数。例如，要获得每个分值类别中最便宜的葡萄酒，我们可以执行以下作：
```pythons
reviews.groupby('points').price.min()
```
![](./images/machineLearn/1746710675068.png)

您可以将我们生成的每个组视为 DataFrame 的一个切片，其中仅包含具有匹配值的数据。我们可以使用 apply() 方法直接访问此 DataFrame，然后我们可以以我们认为合适的任何方式操作数据。例如，这是从数据集中的每个酒庄中选择第一款评论的葡萄酒名称的一种方法：

```python
reviews.groupby('winery').apply(lambda df: df.title.iloc[0])
```
![](./images/machineLearn/1746710832785.png)


要进行更精细的控制，您还可以按多个列进行分组。例如，以下是我们如何按国家和省份挑选最好的葡萄酒：

```python
reviews.groupby(['country', 'province']).apply(lambda df: df.loc[df.points.idxmax()])
```

![](./images/machineLearn/1746711010617.png)

另一个值得一提的 groupby() 方法是 agg()，它允许你同时在 DataFrame 上运行一堆不同的函数。例如，我们可以生成数据集的简单统计摘要，如下所示：

```python
reviews.groupby(['country']).price.agg([len, min, max])
```

![](./images/machineLearn/1746711148773.png)
有效使用 groupby() 将允许您使用数据集做许多非常强大的事情。

### 多索引（Multi-indexes 类似联合索引）

到目前为止，在我们看到的所有示例中，我们一直在使用具有单标签索引的 DataFrame 或 Series 对象。groupby() 略有不同，因为根据我们运行的操作，它有时会导致所谓的多索引。


多索引与常规索引的不同之处在于它是由多个列组合成的。例如：

```pythons
countries_reviewed = reviews.groupby(['country', 'province']).description.agg([len])
countries_reviewed
```
![](./images/machineLearn/1746711313276.png)

其实根据索引类型也可以看出

```python
mi = countries_reviewed.index
type(mi)
```

![](./images/machineLearn/1746711449114.png)

多索引有几种处理其分层结构的方法，而单级索引则没有。多索引需要两级标签来检索值。处理多索引输出是 pandas 新手经常出现问题的地方。

pandas 文档的 MultiIndex/Advanced Selection 部分详细介绍了多索引的使用案例以及使用它们的说明。
但是，通常您最常使用的多索引方法是转换回常规索引，即 reset_index() 方法：

```python
countries_reviewed.reset_index()
```
![](./images/machineLearn/1746711597070.png)

### 排序

再次查看 countries_reviewed，我们可以看到分组按索引顺序返回数据，而不是按值顺序返回数据。也就是说，当输出 groupby 的结果时，行的顺序取决于索引中的值，而不是数据中的值。

要按照预期的顺序获取数据，我们可以自己排序。而 sort_values() 方法对此非常方便。

```python
countries_reviewed = countries_reviewed.reset_index()
countries_reviewed.sort_values(by='len')
```

![](./images/machineLearn/1746711737401.png)
sort_values() 默认为升序排序，其中最小值排在最前面。但是，大多数时候我们想要降序排序，其中较大的数字在前。这是这样的：

```python
countries_reviewed.sort_values(by='len', ascending=False)
```

![](./images/machineLearn/1746711849782.png)

要按索引值排序，请使用配套的 sort_index()方法。此方法具有相同的参数和默认顺序：

```python
countries_reviewed.sort_index()
```

最后，要知道您可以一次按多个列排序：

```python
countries_reviewed.sort_values(by=['country', 'len'])
```
![](./images/machineLearn/1746711987888.png)

## Data Types and Missing Values（数据类型和缺失值）
中DataFrame 或 Series 中列的数据类型称为 dtype。
可以使用 dtype 属性来获取特定列的类型。 例如，我们可以获取 reviews DataFrame 中 price 列的 dtype：
```python
reviews.price.dtype
```

或者，对整个DataFrame使用dtypes 属性会返回 DataFrame 中每一列的 dtype：
```python
reviews.dtypes
```
![](./images/machineLearn/1746771474072.png)

从数据类型我们可以看出 pandas 如何在内部存储数据。float64 表示它使用 64 位浮点数;int64 表示大小相似的整数，依此类推。
还有一个特点是完全由字符串组成的列没有自己的类型;而且它们被赋予 object 类型。
只要转换是合法有意义的，那么我们可以使用 astype() 函数将一种类型的列转换为另一种类型的列。例如，我们可以将 points 列从其现有的 int64 数据类型转换为 float64 数据类型：
```python
reviews.points.astype('float64')
```
![alt text](./images/machineLearn/1746772534141.png)

当然DataFrame 或 Series 索引也有自己的 dtype：
```python
reviews.index.dtype
```
![alt text](./images/machineLearn/1746772661073.png)
Pandas 还支持更多奇特的数据类型，例如分类数据和时间序列数据。

## Missing data  缺失数据
条目缺失值的值为 NaN，即"Not a Number"的缩写。由于技术原因，这些 NaN 值始终是 float64 dtype。
Pandas 提供了一些特定于缺失数据的方法。用于处理选择 NaN 条目，您可以使用 pd.isnull() （或其配套 pd.notnull()）。这是这样使用的：
```python
reviews[pd.isnull(reviews.country)]
```
![](./images/machineLearn/1746772916379.png)
替换缺失值是一种常见操作。 Pandas 为此问题提供了一个非常方便的方法：fillna()。fillna() 提供了几种不同的策略来缓解此类数据。例如，我们可以简单地将每个 NaN 替换为 "Unknown"：
```python
reviews.region_2.fillna("Unknown")
```
![](./images/machineLearn/1746773025730.png)
或者，我们可以用数据库中给定记录之后某个时间出现的第一个非 null 值来填充每个缺失值。这称为回填策略。

或者，我们可能有一个想要替换的非 null 值。例如，假设自此数据集发布以来，审阅者 Kerin O'Keefe 已将她的 Twitter 用户名从 @kerinokeefe 更改为 @kerino。在数据集中反映这一点的一种方法是使用 replace() 方法对数据进行替换：
```python
reviews.taster_twitter_handle.replace("@kerinokeefe", "@kerino")
```
![](docs/images/machineLearn/1746773178116.png)

这里值得一提的是 replace() 方法，因为它可以方便地替换数据集中给定某种 标记 值。比如缺失数据标志："Unknown"、"Undisclosed"、"Invalid" 等。

## Renaming and Combining 重命名和合并

通常，数据会带有列名、索引名或其他我们不满意的命名约定。在这种情况下，您将学习如何使用 pandas 函数将违规条目的名称更改为更好的名称。
有时候我们能到的数据并不是在一个 DataFrame 或 Series 中，而是需要采用一定规则将多个数据组合在一起进行训练。

### 重命名
我们在这里介绍的第一个函数是 rename()，它允许你更改索引名称和/或列名称。例如，要将数据集中的 points 列更改为 score，我们可以执行以下作：
```python
reviews.rename(columns={'points': 'score'})
```
rename() 允许您通过分别指定 index 或 column 关键字参数来重命名索引或列的名字。它支持多种输入格式，但通常 Python 字典是最方便的。下面是一个使用它来重命名索引的某些元素的示例。
```python
reviews.rename(index={0: 'firstEntry', 1: 'secondEntry'})
```
您可能会经常重命名列，但很少重命名索引值。 因为set_index() 通常更方便。

行索引和列索引都可以自定义 name 属性。推荐使用  rename_axis() 方法更改这些名称。例如：
```python
reviews.rename_axis("wines", axis='rows').rename_axis("fields", axis='columns')
```

### 合并数据

在对数据集执行操作时，我们有时需要组合不同的 DataFrame 和/或 Series。Pandas 有三种核心方法可以做到这一点。按照复杂性增加的顺序，它们是 concat()、join() 和 merge()。merge() 可以做的大部分工作也可以用 join() 更简单地完成，所以我们省略它，在这里专注于前两个函数。

最简单的组合方法是 concat()。给定一个元素列表，此函数将沿轴将这些元素混合在一起。

当我们的数据在不同的 DataFrame 或 Series 对象中但他们具有相同的字段（列）时，这很有用。一个例子是 YouTube 视频数据集，它根据来源国家/地区（例如，在本例中为加拿大和英国）拆分数据。如果我们想同时研究多个国家，我们可以使用 concat() 将它们合并到一起：
```python
canadian_youtube = pd.read_csv("../input/youtube-new/CAvideos.csv")
british_youtube = pd.read_csv("../input/youtube-new/GBvideos.csv")

pd.concat([canadian_youtube, british_youtube])
```
就复杂度而言，join合并器处于中间位置 。join() 允许您组合具有共同索引的不同 DataFrame 对象。例如，要下拉恰好在同一天在加拿大和英国流行的视频，我们可以执行以下操作：
```python
left = canadian_youtube.set_index(['title', 'trending_date'])
right = british_youtube.set_index(['title', 'trending_date'])

left.join(right, lsuffix='_CAN', rsuffix='_UK')
```

此处的 lsuffix 和 rsuffix 参数是必需的，因为数据在英国和加拿大数据集中具有相同的列名。如果不存在重名的列（因为，比如说，我们事先重命名了它们），我们就不需要它们。

# 数据可视化

## 简单入门Seaborn

Seaborn 是一个用于数据可视化的 Python 库。它建立在 matplotlib 之上，并提供了一些非常方便的功能来帮助我们可视化数据。它还提供了一些内置的主题和调色板，可以帮助我们快速创建漂亮的图表。

使用 Seaborn（一款功能强大但易于使用的数据可视化工具）将数据可视化提升到一个新的水平。当然 Seaborn是基于 Python编写的。

简单示例，引入Seaborn：
```python


# Path of the file to read
fifa_filepath = "../input/fifa.csv"

# Read the file into a variable fifa_data
fifa_data = pd.read_csv(fifa_filepath, index_col="Date", parse_dates=True)



# Set the width and height of the figure
plt.figure(figsize=(16,6))

# Line chart showing how FIFA rankings evolved over time 
sns.lineplot(data=fifa_data)


```

![](./images/machineLearn/1747101471316.png)

## 折线图

在之前我们已经使用` sns.lineplot(data=fifa_data)`绘制了图表。其中lineplot方法用来制作折线图。后续我们将使用 sns.barplot 和 sns.heatmap 来制作条形图和热图。
而spotify_data 选择将用于创建图表的数据。

请注意，在创建折线图时，您将始终使用相同的格式，并且新数据集的唯一变化是数据集的名称。 因此，例如，如果您正在使用名为 financial_data 的其他数据集。

有时，我们还要修改其他详细信息，例如图表的大小和图表的标题、图表中的标签等。这些选项中的每一个都可以通过一行代码轻松设置。

```python


# Set the width and height of the figure
plt.figure(figsize=(14,6))

# Add title
plt.title("Daily Global Streams of Popular Songs in 2017-2018")

# Line chart showing daily global streams of each song 
sns.lineplot(data=spotify_data)


```
第一行代码将图形的大小设置为 14 英寸（宽度）x 6 英寸（高度）。 要设置任何图形的大小，您只需复制与显示的相同代码行。 然后，如果您想使用自定义大小，请将提供的 14 和 6 值更改为所需的宽度和高度。

第二行代码设置图窗的标题。 请注意，标题必须始终用引号 （"..."） 括起来！

### 仅绘制部分数据

到目前为止，您已经学习了如何为数据集中的每一列绘制一条线。在本节中，您将学习如何单独绘制某些列。

首先，我们将打印所有列的名称。 
```python
list(spotify_data.columns)
```
在以下代码示例中，我们绘制与数据集中的前两列对应的行。

```python


# Set the width and height of the figure
plt.figure(figsize=(14,6))

# Add title
plt.title("Daily Global Streams of Popular Songs in 2017-2018")

# Line chart showing daily global streams of 'Shape of You'
sns.lineplot(data=spotify_data['Shape of You'], label="Shape of You")

# Line chart showing daily global streams of 'Despacito'
sns.lineplot(data=spotify_data['Despacito'], label="Despacito")

# Add label for horizontal axis
plt.xlabel("Date")


```
前两行代码设置图形的标题和大小（看起来应该非常熟悉！

接下来的两行分别向折线图添加一条线。例如，考虑第一个，它为 “Shape of You” 添加了一行：

* 我们没有设置 data=spotify_data，而是设置 data=spotify_data['Shape of You']。 通常，要只绘制单个列，我们使用这种格式，将列名放在单引号中并将其括在方括号中。 （为了确保您正确指定了列的名称，您可以使用上面学到的命令打印所有列名称的列表。）

* 我们还添加了 label="Shape of You" 以使该行出现在图例中并设置其相应的标签。

最后一行代码修改了水平轴（或 x 轴）的标签，其中所需的标签放在引号 （"..."） 中。

## 条形图

创建条形图的代码如下：
```python
# Set the width and height of the figure
plt.figure(figsize=(10,6))

# Add title
plt.title("Average Arrival Delay for Spirit Airlines Flights, by Month")

# Bar chart showing average arrival delay for Spirit Airlines flights by month
sns.barplot(x=flight_data.index, y=flight_data['NK'])

# Add label for vertical axis
plt.ylabel("Arrival delay (in minutes)")
```
方法`sns.barplot(x=flight_data.index, y=flight_data['NK'])`用于创建条形图。
* x=flight_data.index - 这指定了在水平轴上使用什么。 在本例中，我们选择了为行编制索引的列（在本例中为包含月份的列）。
* y=flight_data['NK'] - 这将设置数据中将用于确定每个条形高度的列。 在本例中，我们选择 'NK' 列。
重要提示：您必须使用 flight_data.index 指定Month列为索引列，不能使用 flight_data['Month'] （这将返回错误）。 这是因为当我们加载数据集时，"Month" 列作为行索引。 我们总是必须使用这种特殊的表示法来选择索引列，对于其他列不影响。

## 热图（Heatmap）
我们还有一种绘图类型需要了解：热图！

在下面的代码单元中，我们创建了一个热图来快速可视化 flight_data 中的模式。 每个单元格都根据其相应的值进行颜色编码。

```python
# Heatmap showing average arrival delay for each airline by month
sns.heatmap(data=flight_data, annot=True)
```
* sns.heatmap - 这会告知笔记本我们要创建热图。
* data=flight_data - 这告诉笔记本我们要使用flight_data 中的所有数据来创建热图。
* annot=True - 这会告诉笔记本在每个单元格中显示值。 如果我们不想显示值，我们可以将其设置为 False。


## 散点图

要创建一个简单的散点图，使用 sns.scatterplot 命令并指定以下值：
* 水平 x 轴 （x=insurance_data['bmi']），以及
* 垂直 y 轴 （y=insurance_data['charges']）。
```python
sns.scatterplot(x=insurance_data['bmi'], y=insurance_data['charges'])
```
![](./images/machineLearn/1747127533100.png)
上面的散点图表明，体重指数 （BMI） 和保险费用呈正相关，其中 BMI 较高的客户通常也倾向于支付更多的保险费用。（这种模式是有道理的，因为高 BMI 通常与较高的慢性病风险相关。）但是我们直接看可能不够直观。
要仔细检查此关系的强度，我们可能希望添加一条回归线或最适合数据的线。 我们通过将命令更改为 sns.regplot 来执行此操作。
```python
sns.regplot(x=insurance_data['bmi'], y=insurance_data['charges'])
```

![](./images/machineLearn/1747127687854.png)

### 带颜色的散点图

我们可以使用散点图来显示三个变量之间的关系（不是两个）,通过对点进行颜色编码。

例如，要了解吸烟如何影响 BMI 和保险费用之间的关系，我们可以用 'smoker' 对点进行颜色编码，并在轴上绘制其他两列 （'bmi'， 'charges'）。

```python
sns.scatterplot(x=insurance_data['bmi'], y=insurance_data['charges'], hue=insurance_data['smoker'])
```
![](./images/machineLearn/1747128413400.png)
此散点图显示，虽然非吸烟者往往会随着 BMI 的增加而支付略多的费用，但吸烟者支付的费用要高得多。
为了进一步强调这一事实，我们可以使用 sns.lmplot 命令添加两条回归线，分别对应于 smokers 和 nonsmokers。 （您会注意到，相对于非吸烟者的回归线，吸烟者的回归线的斜率要陡峭得多！
```python
sns.lmplot(x="bmi", y="charges", hue="smoker", data=insurance_data)
```
![](./images/machineLearn/1747128347085.png)

上面的 sns.lmplot 命令的工作方式与您目前了解的命令略有不同：
* 我们不是设置 x=insurance_data['bmi'] 来选择 insurance_data 中的 'bmi' 列，而是设置 x="bmi" 来仅指定列的名称。
* 同样，y="charges" 和 hue="smoker" 也包含列的名称。
* 我们使用 data=insurance_data 指定数据集。


最后，还需要了解另一种绘图，它可能看起来与之前查看散点图的方式略有不同。 通常，我们使用散点图来突出显示两个连续变量之间的关系（如 "bmi" 和 "charges"）。 但是，我们可以调整散点图的设计，使其在其中一个主轴上具有分类变量（如 "smoker"）。 我们将这种绘图类型称为分类散点图，并使用 sns.swarmplot 命令构建它。
```python
sns.swarmplot(x=insurance_data['smoker'],
              y=insurance_data['charges'])
```
![](./images/machineLearn/1747129010656.png)
该图向我们展示了：
* 平均而言，非吸烟者收取的费用低于吸烟者，并且
* 支付最多的顾客是吸烟者;而支付最少的顾客是非吸烟者。
## 分析数据分布情况

为了方便我们更快速的查看出数据之间的关联关系，因此我们会采用分布图去分析数据，其中最经常使用的就是：直方图和密度图。

我们将使用 150 种不同花的数据集，或者来自三种不同鸢尾花（鸢尾花、花斑鸢尾和鸢尾花）的 50 朵花，进行模拟。

### 直方图

假设我们想创建一个直方图来查看鸢尾花的花瓣长度如何变化。 我们可以使用 sns.histplot 方法来做到这一点。

```python
# Histogram 
sns.histplot(iris_data['Petal Length (cm)'])
```
![](./images/machineLearn/1747229815403.png)

### 密度图

下一种类型的图是核密度估计 （KDE） 图。如果您不熟悉 KDE 绘图，您可以将其视为平滑直方图。
要制作 KDE 绘图，我们使用 sns.kdeplot 命令。 设置 shade=True 为曲线下方的区域填充颜色（data= 选择我们想要绘制的列）。
```python
# KDE plot 
sns.kdeplot(data=iris_data['Petal Length (cm)'], shade=True)
```

###  2D KDE 图

在创建 KDE 绘图时，我们并不局限于单个列。 我们可以使用 sns.jointplot 命令创建二维 （2D） KDE 图。

在下图中，颜色编码向我们展示了我们看到萼片宽度和花瓣长度的不同组合的可能性，其中图形密集的部分表示更有可能出现。

```python
# 2D KDE plot
sns.jointplot(x=iris_data['Petal Length (cm)'], y=iris_data['Sepal Width (cm)'], kind="kde")
```

![](./images/machineLearn/1747230114937.png)

需要注意的是，除了中间的 2D KDE 图外，

* 图顶部的曲线是 x 轴上数据的 KDE 图（在本例中为 iris_data['Petal Length (cm)']），并且
* 图右侧的曲线是 y 轴上数据的 KDE 图（在本例中为 iris_data['Sepal Width (cm)']）。

### 带颜色的分布图
之前我们绘制直方图时，仅绘制了x轴，但是基于Petal Length (cm)其实我们可以区分不同植物种类进行分别展示。
即我们可以使用 sns.histplot 方法来创建花瓣长度的三个不同的直方图（每个物种一个颜色）。

```python
# Histograms for each species
sns.histplot(data=iris_data, x='Petal Length (cm)', hue='Species')

# Add title
plt.title("Histogram of Petal Lengths, by Species")
```

![](./images/machineLearn/1747230398631.png)

我们还可以使用 sns.kdeplot 为每个物种创建一个 KDE 图（如上所述）。 data、x 和 hue 的功能与我们上面使用 sns.histplot 时相同。 此外，我们设置 shade=True 为每条曲线下方的区域着色。

```python
# KDE plots for each species
sns.kdeplot(data=iris_data, x='Petal Length (cm)', hue='Species', shade=True)

# Add title
plt.title("Distribution of Petal Lengths, by Species")
```

![](./images/machineLearn/1747230507405.png)

## 选择合适的图

![](./images/machineLearn/image copy 9.png)

由于我们有多种方式去展示数据，因此决定如何最好地展示出数据背后的关系并不总是那么容易，因此我们将图表类型分为三大类来帮助解决这个问题。

* 趋势  趋势指的就是数据变化的走向
    *  sns.lineplot - 折线图最适合显示一段时间内的趋势，而多条线可用于显示多个组中的趋势。
* 关系 您可以使用许多不同的图表类型来了解数据中变量之间的关系。
    * sns.barplot - 条形图可用于比较对应于不同组的数量。
    * sns.heatmap - 热图可用于在数字表中基于颜色查找出数据的模式。
    * sns.scatterplot - 散点图显示两个连续变量之间的关系;如果用颜色编码，我们还可以显示与第三个分类变量的关系。
    * sns.regplot - 在散点图中包含回归线可以更轻松地查看两个变量之间的任何线性关系。
    * sns.lmplot - 如果散点图包含多个颜色编码的组，则此命令可用于绘制多条回归线。
    * sns.swarmplot - 分类散点图显示连续变量和分类变量之间的关系。
* 数据分布情况 将数据进行可视化分布，以显示我们可以期望在变量中看到的可能值以及它们的可能性。
    * sns.histplot - 直方图显示单个数值变量的分布。
    * sns.kdeplot - KDE 图（或 2D KDE 图）显示单个数值变量（或两个数值变量）的概率估计进行平滑分布。
    * sns.jointplot - 此命令可用于同时显示 2D KDE 图和每个变量的相应 KDE 图。
### 自定义图标风格

seaborn的所有方法都为每个绘图提供默认样式。但是，您可能会发现自定义绘图的外观很有用，这可以通过多添加一行代码来实现！

我们可以使用以下代码快速设置图标主题：
```python
# Change the style of the figure to the "dark" theme
sns.set_style("dark")

# Line chart
plt.figure(figsize=(12,6))
sns.lineplot(data=spotify_data)
```
Seaborn 有五个不同的主题：（1）"darkgrid"、（2）"whitegrid"、（3）"dark"、（4）"white" 和 （5）"ticks"，你只需要使用类似于上面代码单元格中的命令（填写了所选主题）来更改它。
