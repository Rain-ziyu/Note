# 深度学习入门

近年来，人工智能领域一些最令人印象深刻的进步就是在深度学习领域。深度学习模型在自然语言翻译（Natural Language Translation）、图像识别（Image Recognition）和游戏博弈（Game Playing）等领域已经达到甚至超越人类水平的表现。

那么什么是深度学习呢？

深度学习是机器学习的一个分支，其核心特征是通过**多层（“深度”）计算堆叠来学习数据**的复杂表征。这种深度计算结构使深度学习模型能够解析现实世界数据中错综复杂的层级化模式（如图像的边缘→纹理→物体部件→完整物体，或语言的音素→单词→句子→语义），从而在诸多任务上取得突破性表现。

其中多层（“深度”）计算堆叠来学习数据指的是：    模型由多个非线性变换层（如全连接层、卷积层、注意力层）堆叠而成，每一层逐步提取更高层次的抽象特征。例如：图像识别中，浅层可能检测边缘，深层可能识别物体类别。

神经网络通过其强大的连接复杂性和可扩展性，成为深度学习的标志性模型。尽管单个神经元仅执行简单计算，但大量神经元通过多层次、高灵活性的互联，能够学习数据中极其复杂的模式。

深度学习简单来说就是为了解决在机器学习中的特征工程问题。深度学习通过构建多层神经网络来自动提取特征，从而减少了对人工特征工程的依赖。

往往大家会讲深度学习过程当成一个黑盒子，这也是因为深度学习的内部逻辑是非常复杂的。实际上深度学习的核心就是神经网络。神经网络是由大量的神经元（节点）组成的，每个神经元通过连接权重与其他神经元相连。通过调整这些权重，神经网络可以学习到输入数据的特征，从而进行分类、回归等任务。

使用 Keras 和 Tensorflow，学习如何进行：
* 创建**完全连接**的神经网络架构 
    （ 也称为 Dense Layer（密集层） 或 Linear Layer（线性层）。每一层的 每个神经元 都与下一层的 所有神经元 相连。例如：如果第 1 层有 3 个神经元，第 2 层有 2 个神经元，那么它们之间的连接权重矩阵就是 3×2 大小，该矩阵用于后续进行层与层，神经元之间的权重。）
* 运用神经网络解决经典的**回归与分类问题**
* 使用随机梯度下降法训练神经网络
* 使用 dropout、批量归一化和其他技术来提高性能。

## 线性单元

让我们从神经网络的基本组件开始：单个神经元。如图所示，具有一个输入的神经元（或单元）如下所示：
![](./images/deepLearn/mfOlDR6.png)

输入为 x。它与神经元的连接有一个权重，即 w。每当值流经连接时，您都会将该值乘以连接的权重。对于输入 x，到达神经元的是 w * x。神经网络通过修改其权重来“学习”。我们也常称这一个线性单元为**线性函数**。

b 是一种特殊的权重，我们称之为 bias（偏差）。偏差没有任何与之关联的输入数据;相反，我们在图中放置一个 1，这样到达神经元的值就是 b（因为 1 * b = b）。偏差使神经元能够独立于其输入修改输出。
y 是神经元最终输出的值。为了获得输出，神经元将它通过其连接接收到的所有值相加。这个神经元的激活是 y = w * x + b，或者作为公式 y=wx+by=wx+by = w x + b。
### 线性函数：

用于计算一个数据在每一个类别上的最终得分

f(x,W) = W* x+b 

其中W表示每一个数据点（将一个数据分成多个部分作为基本单元进行计算）对应的权重系数，x表示所有数据点对应的值，b表示每一种分类计算值是需要的浮动量。

下图是以一张图片为例，图片粗略的分为四个像素点，实际上像素点肯定更多，然后这是一个三分类的示例。
![](./images/deepLearn/image.png)

### 线性函数计算过程示例；
尽管单个神经元通常仅作为更大网络的一部分发挥作用，但从单个神经元模型作为基础开始学习通常很有用。单神经元模型其实就是线性模型。
让我们以"80种谷物"数据集为例来理解这个过程。假设我们训练一个模型，以"含糖量"（每份糖的克数）作为输入，"卡路里"（每份的热量）作为输出，最终得到模型的偏置项b=90，权重w=2.5。那么对于一份含5克糖的谷物，我们可以这样估算其卡路里含量：
![](./images/deepLearn/yjsfFvY.png)
而且，根据我们的公式，我们有 calories = 2.5 × 5 + 90 = 102.5 卡路里=2.5×5+90=102.5 ，正如我们预期的那样。

### 多个输入
上文使用的卡路里数据集其中有很多其他特征不仅仅是含糖量。如果我们想扩展我们的模型以包括纤维或蛋白质含量等内容，该怎么办？这很简单。我们可以向神经元添加更多的输入连接，每个额外的特征对应一个连接权重。具体计算时，我们将每个输入乘以其连接权重，然后将它们全部相加。
![](./images/deepLearn/vyXSnlZ.png)

这个神经元最终的公式是 `y=w0*x0+w1*x1+w2*x2+b`.具有两个输入的线性单元将拟合一个二维平面，而具有多个输入的单元将拟合一个多维超平面。

### Keras中的线性单元
在 Keras 中创建模型的最简单方法是通过 keras.Sequential，它将神经网络创建为层堆栈。我们可以使用 dense 层创建上述模型（我们将在下一课中详细介绍）。

我们可以定义一个线性模型，接受三个输入特征（'sugars'、'fiber' 和 'protein'）并产生一个输出 （'calories'），如下所示：
```python
from tensorflow import keras
from tensorflow.keras import layers

# Create a network with 1 linear unit
model = keras.Sequential([
    layers.Dense(units=1, input_shape=[3])
])
```

第一个参数 units，用于定义我们想要多少个输出。在这种情况下，我们只预测 'calories'，因此我们将使用 units=1。
第二个参数 input_shape，我们告诉 Keras 输入的维度。设置 input_shape=[3] 可确保模型接受三个特征作为输入（'sugars'、'fiber' 和 'protein'）。

一个简单的神经单元模型就建立完成了。

> 为什么input_shape是一个 Python 列表？
> 我们将在后续演示中使用的数据将是表格数据，就像在 Pandas dataframe中一样。数据集中的每个特征都有一个输入。特征按列排列，因此我们需要使用 input_shape=[num_columns] Keras 在此处使用列表的原因是允许使用更复杂的数据集。例如，图像数据可能需要三个维度：[height, width, channels]


Keras 使用张量存储网络所有权重参数，其本质是 TensorFlow 优化的多维数组（类似 NumPy 数组但更强大） 

模型的权重作为张量列表保存在其 weights 属性中。获取您在上面定义的模型的权重。（如果需要，您可以使用类似以下内容来显示权重：
```python
# YOUR CODE HERE
w, b = model.weights
print("Weights\n{}\n\nBias\n{}".format(w, b))
# Check your answer
q_3.check()
```
![](./images/deepLearn/1748440903971.png)
与 NumPy 数组的关键区别

| 特性         | NumPy 数组     | TensorFlow 张量               |
|--------------|----------------|-------------------------------|
| **硬件加速** | 仅 CPU         | 支持 GPU/TPU                 |
| **自动微分** | 不支持         | 原生支持 (`GradientTape`)    |
| **分布式计算**| 手动实现       | 原生并行化                   |
| **计算图优化**| 无             | XLA 编译器优化               |

需要注意的是在训练之前，模型的权重是随机设置的。运行下面的单元格几次，以查看不同行生成的的随机初始化权重值。
```python
import tensorflow as tf
import matplotlib.pyplot as plt

model = keras.Sequential([
    layers.Dense(1, input_shape=[1]),
])

x = tf.linspace(-1.0, 1.0, 100)
y = model.predict(x)

plt.figure(dpi=100)
plt.plot(x, y, 'k')
plt.xlim(-1, 1)
plt.ylim(-1, 1)
plt.xlabel("Input: x")
plt.ylabel("Target y")
w, b = model.weights # you could also use model.get_weights() here
plt.title("Weight: {:0.2f}\nBias: {:0.2f}".format(w[0][0], b[0]))
plt.show()
```
![](./images/deepLearn/1748440853705.png)
## 深度神经网络

在本节中，我们将探索如何构建能够学习深度神经网络所擅长的复杂关系的神经网络。

这里的关键思想是模块化，从更简单的功能单元构建一个复杂的网络。我们之前已经了解了线性单元如何计算线性函数 -- 现在我们将了解如何组合和修改这些单个单元来模拟更复杂的关系。

### Layers 层

神经网络通常将其神经元组织成层。当我们将具有一组公共输入的线性单元收集在一起时，我们会得到一个密集层。

下图是一个由两个线性单元组成的密集层，接收两个输入和一个偏差。
![](./images/deepLearn/2MA4iMV.png)

您可以将神经网络中的每一层都视为执行某种相对简单的转换。通过深层堆栈，神经网络可以以越来越复杂的方式转换其输入。在训练有素的神经网络中，每一层都是一个转换，让我们更接近一个解决方案。

> 多种多样的层
> 在 Keras 中，“层”是一种非常通用的东西。从本质上讲，层可以是任何类型的数据转换。许多层，如卷积层和递归层，通过使用神经元来转换数据，主要区别在于它们形成的连接模式。其他 API 使用特征工程或仅进行简单的算术。这一个充满各种层次的世界等着你去发现 - 快来看看吧！

### 激活函数

然而，事实证明，两个中间没有任何内容的密集层并不比单个密集层本身好。密集的图层本身永远无法让我们走出线条和平面的世界。我们需要的是非线性的东西。也就是激活函数。

可以说如果没有激活函数，神经网络只能学习线性关系。为了拟合曲线，我们需要使用激活函数。
![](./images/deepLearn/OLSUEYT.png)

激活函数只是我们应用于层的每个输出（其激活）的某个函数。最常见的是max(0,x) 整流器函数.

![](./images/deepLearn/aeIyAlF.png)

整流函数的图形是一条直线，但负值部分会被"修正"为零。将这个函数应用于神经元的输出时，会在数据中产生一个转折点，从而使我们摆脱简单的线性关系。

当我们将整流器连接到线性单元时，我们会得到一个整流线性单元或 ReLU。（因此，通常将整流器函数称为“ReLU 函数”。 将 ReLU 激活应用于线性单元意味着输出变为 max(0, w * x + b)，我们可以将其绘制成下图：
![](./images/machineLearn/eFry7Yu.png)

### 堆叠全连接层
现在我们已经了解了一些非线性的特征，让我们看看如何堆叠层来获得复杂的数据转换。
下图是一个结构示例：堆叠全连接层 即构成"全连接网络"
![](./images/deepLearn/Y5iwFQZ.png)

输出层之前的层有时被称为隐藏层，因为我们无法直接看到它们的输出。

需要注意的是，上图中的最终 （输出） 层是一个线性单元 （意味着没有激活函数）。这使得这个网络适合于回归任务，用于试图预测一些可以是任意的数值任务。其他任务（如分类）可能需要对输出执行激活函数。

### Building Sequential Models （构建 Sequential 模型）

我们一直在使用的 Sequential 模型将按从头到尾的顺序将一系列层连接在一起：第一层获得输入，最后一层产生输出。下面是对应模型的代码示例：
```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    # the hidden ReLU layers
    layers.Dense(units=4, activation='relu', input_shape=[2]),
    layers.Dense(units=3, activation='relu'),
    # the linear output layer 
    layers.Dense(units=1),
])
```
需要注意的是，所有的层是放到一个列表中，例如
```python
[
    # the hidden ReLU layers
    layers.Dense(units=4, activation='relu', input_shape=[2]),
    layers.Dense(units=3, activation='relu'),
    # the linear output layer 
    layers.Dense(units=1),
]
```
而不是作为单独的参数。要配置层使用的激活函数，只需在层中添加一个 activation 参数即可。对于输出层，我们不需要激活函数，因为我们正在执行回归任务。

## 随机梯度下降法

之前我们学习了如何通过堆叠全连接层来构建全连接网络。网络首次创建时，网络的所有权重都是随机设置的 -- 网络还一无所知。在本节中，我们将了解如何训练神经网络，了解观察神经网络的学习过程。

与所有机器学习任务一样，我们从一组训练数据开始。训练数据中的每个样本都包含一些特征 （输入） 和预期目标 （输出）。训练神经网络其实就是调整其权重，使其能够将特征转换为目标。例如，在 80 个谷物数据集中，我们想要得到一个神经网络，它可以获取每种谷物的 'sugar'、'fiber' 和 'protein' 含量，并对该谷物的 'calories' 产生预测。如果我们能成功地训练一个网络来做到这一点，那么它的权重必须以某种方式表示这些特征与训练数据中表达的目标之间的关系。

除了训练数据之外，我们还需要两样东西：
* 一个 “损失函数”，用于衡量网络预测的好坏。
* 一个 “优化器” ，可以告诉网络如何更改其权重。

### 损失函数

我们已经了解了如何设计网络架构，但尚未探讨如何让网络明确要解决的具体问题。这正是损失函数的作用所在。

损失函数通过测量目标的真实值与模型预测的值之间的差异。

针对不同的问题需要采用不同的损失函数。我们一直在探讨回归问题，这类问题的目标是预测数值型结果，比如预测80种谷物的卡路里含量、红酒品质评分等。其他回归任务可能还包括预测房价或汽车燃油效率等。

回归问题的常见损失函数是平均绝对误差或 MAE。对于每个预测y_pred 通过绝对差值 abs(y_true - y_pred) 来衡量与真实目标y_true的差异。

数据集上的总 MAE 损失是所有这些绝对差异的平均值。

![](./images/deepLearn/VDcvkZN.png)

除了 MAE 之外，您可能会看到的回归问题的其他损失函数是均方误差 （MSE） 或 Huber 损失（在 Keras 中均可用）。

在训练期间，模型将使用 loss 函数作为指南来查找其权重的正确值（loss 越低越好）。换句话说，损失函数会告诉神经网络它的目标。


### 优化器

我们已经描述了我们希望神经网络解决的问题，但现在我们需要说明如何解决它。这是优化器的工作。优化器是一种调整权重以最小化损失的算法。

实际上，深度学习中使用的所有优化算法都属于一个称为随机梯度下降的家族。它们是逐步训练网络的迭代算法。训练的步骤是这样的：

1. 对一些训练数据进行采样，并通过网络运行它以进行预测。
2. 测量预测值与真实值之间的损失。
3. 最后，沿使损失较小的方向调整权重。

然后一遍又一遍地这样做，直到损失达到你想要的程度（或者直到它不会进一步减少）。

![](./images/deepLearn/rFI1tIk.gif)

上图是使用 Stochastic Gradient Descent 训练神经网络的过程。

每次迭代所使用的训练数据样本称为"小批量"(minibatch，通常简称为"batch")，而完整遍历一次全部训练数据则称为一个"epoch"(训练周期)。设置的epoch数量决定了网络将会看到每个训练样本的次数。

上面的这段动画展示了第一节中的线性模型通过随机梯度下降(SGD)进行训练的过程。浅红色圆点代表整个训练集，而实心红点则表示当前使用的小批量数据。每当SGD处理一个新批次时，它都会调整权重参数（w表示斜率，b表示截距），使其在该批次数据上趋向最佳值。经过连续批次的迭代，回归线最终收敛至最优拟合状态。可以观察到，随着权重值逐渐接近真实参数，损失值也在持续减小。

### Learning Rate and Batch Size  学习率和批量大小

可以注意到，回归线仅会朝每个批次的方向进行微调（而非完全匹配）。这些调整的幅度由学习率决定：较小的学习率意味着网络需要处理更多小批量数据才能使权重收敛到最优值。

学习率与小批量大小是影响SGD训练过程的两个最关键参数。它们之间的相互作用往往十分微妙，要确定这些参数的最佳选择并不总是显而易见的（我们将在练习中探讨这些影响）。

值得庆幸的是，对于大多数应用场景，并不需要进行大量超参数搜索就能获得令人满意的结果。Adam作为一种自适应学习率的SGD算法，无需参数调优就适用于绝大多数问题（从某种意义上说它是"自调节"的）。Adam堪称通用优化器的理想之选。

### 添加 Loss （损失函数）和 Optimizer（优化器）

在定义模型后，您可以使用模型的 compile 方法添加损失函数和优化器：
```python
model.compile(
    optimizer="adam",
    loss="mae",
)
```
需要特别说明的是，我们仅需通过字符串即可指定损失函数和优化器。虽然您也可以通过Keras API直接调用这些模块（例如需要进行参数调优时），但在当前场景下，默认参数配置已完全够用。
> 术语解析
> 梯度（gradient）本质上是一个矢量，其方向指示了权重参数的最优调整方向。更准确地说，梯度表征了如何调整权重才能使损失值以最快速度变化。我们将这一优化过程称为梯度下降（gradient descent），因为算法正是沿着梯度方向在损失曲面上向极小值点"下降"。
> "随机"（Stochastic）一词的本意是"由概率决定"。之所以称该训练过程具有随机性，是因为每次迭代使用的小批量数据（minibatch）都是从数据集中随机抽样得到的——这正是"SGD"（随机梯度下降）名称的由来！

#### 基于红葡萄酒品质数据集进行示例

现在，我们知道了开始训练深度学习模型所需的一切。那么让我们看看它的实际效果吧！我们将使用 Red Wine Quality 数据集。

该数据集由大约 1600 种葡萄牙红葡萄酒的理化测量值组成。还包括对每款葡萄酒进行盲品测试的质量评级。我们从这些测量中预测葡萄酒的感知质量的能力如何？

数据示例如下：
![](./images/deepLearn/1748526232024.png)

这个神经网络应该有多少个输入？我们可以通过查看dataframe中的列数来发现这一点。确保此处不要包含目标 （'quality'） —— 只包含输入特征。

```python
print(X_train.shape)
```

(1119, 11)

我们得到11 列表示 11 个输入。

在这里我们选择了一个具有 1500 多个神经元的三层网络。这个网络应该能够学习数据中相当复杂的关系。
```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(512, activation='relu', input_shape=[11]),
    layers.Dense(512, activation='relu'),
    layers.Dense(512, activation='relu'),
    layers.Dense(1),
])
```

模型架构的确定应当遵循系统化的设计流程。建议从简单结构入手，并以验证损失（validation loss）作为核心评估指标。后续练习环节将深入讲解模型开发的完整方法论。

在模型定义完成后，需要通过编译环节集成优化器与损失函数。
```python
model.compile(
    optimizer='adam',
    loss='mae',
)
```
现在我们准备好开始训练了！我们告诉 Keras 一次向优化器提供 256 行训练数据（batch_size），并在整个数据集（epochs）中执行 10 次。

```python
history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=256,
    epochs=10,
)
```
![](./images/deepLearn/1748526620394.png)
您会注意到，Keras在模型训练过程中将持续更新损失值的变化情况。

通常，查看损失的更好方法是为其绘制图表。fit 方法实际上在 History 对象中保留了训练期间产生的损失的记录。我们将数据转换为 Pandas 数据帧，这样可以轻松绘图。

![](./images/deepLearn/1748526778132.png)

可以观察到，随着训练轮次（epoch）的增加，损失值逐渐趋于平稳。当损失曲线呈现这种水平状态时，表明模型已达到最佳学习效果，继续增加训练轮次已无必要。

> 学习率和批量大小
```python
# YOUR CODE HERE: Experiment with different values for the learning rate, batch size, and number of examples
learning_rate = 0.05
batch_size = 32
num_examples = 256

animate_sgd(
    learning_rate=learning_rate,
    batch_size=batch_size,
    num_examples=num_examples,
    # You can also change these, if you like
    steps=50, # total training steps (batches seen)
    true_w=3.0, # the slope of the data
    true_b=2.0, # the bias of the data
)
```
以上代码会生成一个视频，来演示更改 learning_rate、batch_size 和 num_examples 的值会产生的效果。

您可能已经注意到，较小的批量大小会导致权重更新和损失曲线波动更大。这是因为每个批次只是数据的一个小样本，而样本越小，其估计值往往波动性越大。不过，较小的批量也可能产生某种"平均化"效应，这反而可能带来益处。

较小的学习率会使更新幅度变小，导致训练收敛所需时间更长。较大的学习率可以加速训练过程，但难以"稳定"在最小值附近。当学习率过大时，训练可能会完全失败。（您可以尝试将学习率设为0.99这样的大值来观察这种现象。）

## 过拟合和欠拟合

之前我们提到Keras 将保留其正在训练模型的 epoch 内的训练和验证损失的历史记录。在本课中，我们将学习如何解释这些学习曲线，以及如何使用它们来指导模型开发。特别是，我们将检查学习曲线中是否存在欠拟合和过拟合的证据，并研究几种纠正它的策略。

### 解释Learning Curves学习曲线

我们可以将训练数据中的信息视为由两部分组成：信号与噪声。信号是具有泛化能力的部分，能帮助模型对新数据做出预测；而噪声则是仅存在于训练数据中的部分——包括现实世界数据中的随机波动，以及那些看似有用实则无意义的偶然模式。噪声可能伪装成有效特征，但实际上对预测毫无帮助。

我们通过选择权重或参数来训练模型，以最大限度地减少训练集上的损失。但是，您可能知道，要准确评估模型的性能，我们需要根据一组新数据（验证数据）对其进行评估。（此时就需要我们之前学习的模型验证方法）

当我们训练模型时，我们一直在逐个 epoch 地绘制训练集上的损失。为此，我们还将添加一个验证数据的绘图。这些图称为学习曲线。为了有效地训练深度学习模型，我们需要能够解释它们。

![](./images/deepLearn/tHiVFnM.png)
验证损失（validation loss）图像可用于预估模型在未知数据上的预期误差。

在模型训练过程中，训练损失（training loss）的下降可能源于两种情形：模型学会了有效信号，或是记住了数据噪声。而验证损失（validation loss）的下降，只有当模型真正掌握可泛化信号时才会出现。（因为模型从训练集学到的任何噪声都无法迁移到新数据上。）

因此，当模型学习到有效信号时，两条曲线会同步下降；但当模型开始记忆噪声时，曲线间就会出现明显间隙。这个间隙的大小，恰恰揭示了模型所学噪声的多少。

理想情况下，我们希望构建的模型能够完全学习有效信号而彻底规避噪声——但这在实践中几乎不可能实现。因此，我们需要进行权衡取舍：通过允许模型学习更多噪声作为代价，换取其对有效信号的更强捕捉能力。只要这个代价收益比对我们有利，验证损失就会持续下降。然而超过某个临界点后，这种权衡就会转为负面效应——当噪声学习的代价超过信号获取的收益时，验证损失便开始上升。

![](./images/deepLearn/eUF6mfo.png)
上图表示出来了欠拟合与过拟合的表现

这种权衡关系揭示了模型训练中可能存在的两大问题：信号学习不足与噪声学习过度。

    欠拟合（Underfitting）：由于模型未能充分捕捉有效信号，导致训练损失未达到理论最低值。

    过拟合（Overfitting）：由于模型过度记忆噪声，反而使得损失无法进一步降低。

深度学习模型训练的核心技巧，就在于找到二者之间的最优平衡点。

下面我们将学习几种从训练数据中获得更多信号同时减少噪声量的方法。

### Capacity（模型容量）
模型的容量（capacity）决定了其能够学习到的模式规模和复杂度。对于神经网络而言，容量主要取决于两个关键因素：神经元数量及其连接方式。当发现网络出现欠拟合现象时，提升模型容量往往是有效的解决方案。

可以通过两种方式提升网络容量：扩展网络宽度（在现有层中增加神经元数量）或增加网络深度（添加更多网络层）。

    宽度优势：拓宽的网络更擅长捕捉线性特征

    深度优势：深层网络则更易于学习非线性关系

具体选择哪种结构更优，完全取决于数据集特性。

```python
model = keras.Sequential([
    layers.Dense(16, activation='relu'),
    layers.Dense(1),
])

wider = keras.Sequential([
    layers.Dense(32, activation='relu'),
    layers.Dense(1),
])

deeper = keras.Sequential([
    layers.Dense(16, activation='relu'),
    layers.Dense(16, activation='relu'),
    layers.Dense(1),
])
```

### 提前停止
前文提到，当模型过度学习噪声时，验证损失（validation loss）在训练过程中可能开始上升。为防止这种情况，我们可以采用一个简单策略：一旦验证损失停止下降，立即终止训练。这种提前中断训练的技术，被称为早停法（Early Stopping）。

![](./images/deepLearn/eP0gppr.png)
上图表示我们最终保存的模型，应处于验证损失最小的状态。

一旦我们检测到验证损失再次开始上升，我们就可以将权重重置回出现最小值的位置。这可确保模型不会继续学习噪声和过度拟合数据。

提前停止训练还意味着我们在网络完成学习信号之前过早停止训练的风险较小。因此，除了防止 过拟合 训练时间过长外，早停还可以防止 欠拟合 训练时间不够长的问题。只需将训练 epoch 设置为较大的数字（大于您需要的数量），提前停止将处理其余工作。

### 如何添加提前停止

在Keras中，我们可以通过回调函数（Callback）机制实现早停法。回调函数是在网络训练过程中定期执行的特定功能模块。其中，早停回调会在每个训练周期（epoch）结束后自动触发。（Keras提供了多种预定义回调工具，同时也支持开发者自定义回调函数。）
```python
from tensorflow.keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(
    min_delta=0.001, # minimium amount of change to count as an improvement
    patience=20, # how many epochs to wait before stopping
    restore_best_weights=True,
)
```
这些参数的设置含义是："若验证损失在连续20个训练周期内提升幅度未达0.001，则终止训练并保留当前最优模型"。由于验证损失的波动可能源于过拟合，也可能来自批数据的随机变异，该机制通过参数设定为我们提供了容错空间：

    灵敏度控制：0.001的阈值过滤无关波动

    观察窗口：20个周期的缓冲期避免过早终止

    智能保留：自动保存历史最佳模型
下面我们将演示如何将早停回调集成到模型训练中。只需在 fit 方法中添加 callbacks 参数即可：

#### 示例

我们将增加该网络的容量，但还会添加一个提前停止回调以防止过度拟合。

现在，我们来增加网络的容量。我们将选择一个相当大的网络，但一旦验证损失显示出增加的迹象，就依靠回调来停止训练。

```python
from tensorflow import keras
from tensorflow.keras import layers, callbacks

early_stopping = callbacks.EarlyStopping(
    min_delta=0.001, # minimium amount of change to count as an improvement
    patience=20, # how many epochs to wait before stopping
    restore_best_weights=True,
)

model = keras.Sequential([
    layers.Dense(512, activation='relu', input_shape=[11]),
    layers.Dense(512, activation='relu'),
    layers.Dense(512, activation='relu'),
    layers.Dense(1),
])
model.compile(
    optimizer='adam',
    loss='mae',
)
```
定义回调后，将其作为参数添加到 fit 中（你可以有多个，所以把它放在一个列表中）。使用提前停止时，请选择大量的 epoch，比您需要的要多。
```python
history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=256,
    epochs=500,
    callbacks=[early_stopping], # put your callbacks in a list
    verbose=0,  # turn off training log
)

history_df = pd.DataFrame(history.history)
history_df.loc[:, ['loss', 'val_loss']].plot();
print("Minimum validation loss: {}".format(history_df['val_loss'].min()))
```

![](./images/deepLearn/__results___7_1.png)
